{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCZt/JErz6A4hsbPGtXxkS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xaomomo/Unet-with-attention/blob/main/LSHAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSH attention with no attention between chunks as keras multihead subclass"
      ],
      "metadata": {
        "id": "Qbiwq0rKHdag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f1d99b-b6b2-43ce-cd75-d11066fc03e4",
        "id": "a-BX8SLoI2Wu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\n",
            "To: /content/input.zip\n",
            "100% 367M/367M [00:04<00:00, 74.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex0pbxLdI2Ww"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ZAoQWII2Ww"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtw5FDriI2Wx"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "import PIL\n",
        "from PIL import Image\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/images/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    X[i,:,:,0]=im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST)\n",
        "mas=\"/Amasks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/Amasks/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    im=np.array(im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST))\n",
        "    im[im==85]=1\n",
        "    im[im==170]=2\n",
        "    im[im==255]=3\n",
        "    Y[i,:,:]=im\n",
        "import tensorflow as tf\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))\n",
        "Y=tf.keras.utils.to_categorical(Y)\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e72c877-0735-4760-8b04-e86077009f42",
        "id": "mu5trMBgI2Wx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3520, 128, 128)\n",
            "[0. 1. 2. 3.]\n",
            "(3520, 128, 128, 4)\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea1c05e-f853-41fc-f235-49cd07199068",
        "id": "XsDeC0BxI2Wy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002\n",
            "003\n",
            "004\n",
            "005\n",
            "006\n",
            "007\n",
            "008\n",
            "009\n",
            "010\n",
            "10_85902_1\n",
            "10_85902_3\n",
            "14_85914_0\n",
            "27_86410_0\n",
            "29_86490_1\n",
            "29_86491_1\n",
            "36_86526_0\n",
            "40_86625_0\n",
            "4_85506_1\n",
            "7_85703_0\n"
          ]
        }
      ],
      "source": [
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "Siz=0\n",
        "id=0\n",
        "ri=0\n",
        "metadat=[]\n",
        "c=\"001\"\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x.find(\"radiopaedia\") == -1:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\")]\n",
        "  else:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\",9)]\n",
        "  if c!=x:\n",
        "    print(x)\n",
        "    metadat.append([id,Siz,ri])\n",
        "    ri=i\n",
        "    c=x\n",
        "    id+=1\n",
        "    Siz=0\n",
        "  Siz+=1\n",
        "metadat.append([id,Siz,ri])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqn8h312I2Wz"
      },
      "source": [
        "##New Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90dd5890-45e4-4458-e8bd-7a783af73e3a",
        "id": "L4YXvn3tI2Wz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  4  5  7  8  9 11 12 13 14 15 17 18 19] [ 3  6 10 16]\n",
            "[ 0  1  3  5  6  7  8  9 10 11 12 13 15 16 18 19] [ 2  4 14 17]\n",
            "[ 2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19] [ 0  1  7 13]\n",
            "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17] [ 9 15 18 19]\n",
            "[ 0  1  2  3  4  6  7  9 10 13 14 15 16 17 18 19] [ 5  8 11 12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "IDX=[x for x in range(10)]\n",
        "IDY=[x for x in range(10)]\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  print(train_index, val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8hWheeI2W0"
      },
      "outputs": [],
      "source": [
        "for i,x in enumerate(train_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_train=X[ri:ri+size]\n",
        "    Y_train=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_train=np.concatenate((X_train,X[ri:ri+size]),axis=0)\n",
        "    Y_train=np.concatenate((Y_train,Y[ri:ri+size]),axis=0)\n",
        "for i,x in enumerate(val_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_test=X[ri:ri+size]\n",
        "    Y_test=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_test=np.concatenate((X_test,X[ri:ri+size]),axis=0)\n",
        "    Y_test=np.concatenate((Y_test,Y[ri:ri+size]),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSH Attention init"
      ],
      "metadata": {
        "id": "-hiaYo2eJJHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHlvgwPMrBYF"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Keras-based multi-head attention layer.\"\"\"\n",
        "\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras.engine.base_layer import Layer\n",
        "from keras.layers import activation\n",
        "from keras.layers import core\n",
        "from keras.layers import regularization\n",
        "from keras.utils import tf_utils\n",
        "\n",
        "# isort: off\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "_CHR_IDX = string.ascii_lowercase\n",
        "\n",
        "def _build_attention_equation(rank, attn_axes):\n",
        "    \"\"\"Builds einsum equations for the attention computation.\n",
        "    Query, key, value inputs after projection are expected to have the shape as:\n",
        "    `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
        "    `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
        "    The attention operations can be generalized:\n",
        "    (1) Query-key dot product:\n",
        "    `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
        "    <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
        "    num_heads, <query attention dims>, <key attention dims>)`\n",
        "    (2) Combination:\n",
        "    `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
        "    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch\n",
        "    dims>, <query attention dims>, num_heads, channels)`\n",
        "    Args:\n",
        "      rank: Rank of query, key, value tensors.\n",
        "      attn_axes: List/tuple of axes, `[-1, rank)`,\n",
        "        that attention will be applied to.\n",
        "    Returns:\n",
        "      Einsum equations.\n",
        "    \"\"\"\n",
        "    target_notation = _CHR_IDX[:rank]\n",
        "    # `batch_dims` includes the head dim.\n",
        "    batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
        "    letter_offset = rank\n",
        "    source_notation = \"\"\n",
        "    for i in range(rank):\n",
        "        if i in batch_dims or i == rank - 1:\n",
        "            source_notation += target_notation[i]\n",
        "        else:\n",
        "            source_notation += _CHR_IDX[letter_offset]\n",
        "            letter_offset += 1\n",
        "\n",
        "    product_notation = \"\".join(\n",
        "        [target_notation[i] for i in batch_dims]\n",
        "        + [target_notation[i] for i in attn_axes]\n",
        "        + [source_notation[i] for i in attn_axes]\n",
        "    )\n",
        "    dot_product_equation = \"%s,%s->%s\" % (\n",
        "        source_notation,\n",
        "        target_notation,\n",
        "        product_notation,\n",
        "    )\n",
        "    attn_scores_rank = len(product_notation)\n",
        "    combine_equation = \"%s,%s->%s\" % (\n",
        "        product_notation,\n",
        "        source_notation,\n",
        "        target_notation,\n",
        "    )\n",
        "    return dot_product_equation, combine_equation, attn_scores_rank\n",
        "\n",
        "\n",
        "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
        "    \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
        "    input_str = \"\"\n",
        "    kernel_str = \"\"\n",
        "    output_str = \"\"\n",
        "    bias_axes = \"\"\n",
        "    letter_offset = 0\n",
        "    for i in range(free_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        input_str += char\n",
        "        output_str += char\n",
        "\n",
        "    letter_offset += free_dims\n",
        "    for i in range(bound_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        input_str += char\n",
        "        kernel_str += char\n",
        "\n",
        "    letter_offset += bound_dims\n",
        "    for i in range(output_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        kernel_str += char\n",
        "        output_str += char\n",
        "        bias_axes += char\n",
        "    equation = f\"{input_str},{kernel_str}->{output_str}\"\n",
        "    return equation, bias_axes, len(output_str)\n",
        "\n",
        "\n",
        "def _get_output_shape(output_rank, known_last_dims):\n",
        "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n",
        "\n",
        "class Linformer(Layer):\n",
        "    \"\"\"MultiHeadAttention layer.\n",
        "    This is an implementation of multi-headed attention as described in the\n",
        "    paper \"Attention is all you Need\" (Vaswani et al., 2017).\n",
        "    If `query`, `key,` `value` are the same, then\n",
        "    this is self-attention. Each timestep in `query` attends to the\n",
        "    corresponding sequence in `key`, and returns a fixed-width vector.\n",
        "    This layer first projects `query`, `key` and `value`. These are\n",
        "    (effectively) a list of tensors of length `num_attention_heads`, where the\n",
        "    corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n",
        "    `(batch_size, <key/value dimensions>, key_dim)`,\n",
        "    `(batch_size, <key/value dimensions>, value_dim)`.\n",
        "    Then, the query and key tensors are dot-producted and scaled. These are\n",
        "    softmaxed to obtain attention probabilities. The value tensors are then\n",
        "    interpolated by these probabilities, then concatenated back to a single\n",
        "    tensor.\n",
        "    Finally, the result tensor with the last dimension as value_dim can take an\n",
        "    linear projection and return.\n",
        "    When using `MultiHeadAttention` inside a custom layer, the custom layer must\n",
        "    implement its own `build()` method and call `MultiHeadAttention`'s\n",
        "    `_build_from_signature()` there.\n",
        "    This enables weights to be restored correctly when the model is loaded.\n",
        "    Examples:\n",
        "    Performs 1D cross-attention over two sequence inputs with an attention mask.\n",
        "    Returns the additional attention weights over heads.\n",
        "    >>> layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
        "    >>> target = tf.keras.Input(shape=[8, 16])\n",
        "    >>> source = tf.keras.Input(shape=[4, 16])\n",
        "    >>> output_tensor, weights = layer(target, source,\n",
        "    ...                                return_attention_scores=True)\n",
        "    >>> print(output_tensor.shape)\n",
        "    (None, 8, 16)\n",
        "    >>> print(weights.shape)\n",
        "    (None, 2, 8, 4)\n",
        "    Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n",
        "    >>> layer = MultiHeadAttention(\n",
        "    ...     num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
        "    >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
        "    >>> output_tensor = layer(input_tensor, input_tensor)\n",
        "    >>> print(output_tensor.shape)\n",
        "    (None, 5, 3, 4, 16)\n",
        "    Args:\n",
        "      num_heads: Number of attention heads.\n",
        "      key_dim: Size of each attention head for query and key.\n",
        "      value_dim: Size of each attention head for value.\n",
        "      dropout: Dropout probability.\n",
        "      use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
        "      output_shape: The expected shape of an output tensor, besides the batch\n",
        "        and sequence dims. If not specified, projects back to the key feature\n",
        "        dim.\n",
        "      attention_axes: axes over which the attention is applied. `None` means\n",
        "        attention over all axes, but batch, heads, and features.\n",
        "      kernel_initializer: Initializer for dense layer kernels.\n",
        "      bias_initializer: Initializer for dense layer biases.\n",
        "      kernel_regularizer: Regularizer for dense layer kernels.\n",
        "      bias_regularizer: Regularizer for dense layer biases.\n",
        "      activity_regularizer: Regularizer for dense layer activity.\n",
        "      kernel_constraint: Constraint for dense layer kernels.\n",
        "      bias_constraint: Constraint for dense layer kernels.\n",
        "    Call arguments:\n",
        "      query: Query `Tensor` of shape `(B, T, dim)`.\n",
        "      value: Value `Tensor` of shape `(B, S, dim)`.\n",
        "      key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n",
        "        `value` for both `key` and `value`, which is the most common case.\n",
        "      attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "        attention to certain positions. The boolean mask specifies which query\n",
        "        elements can attend to which key elements, 1 indicates attention and 0\n",
        "        indicates no attention. Broadcasting can happen for the missing batch\n",
        "        dimensions and the head dimension.\n",
        "      return_attention_scores: A boolean to indicate whether the output should\n",
        "        be `(attention_output, attention_scores)` if `True`, or\n",
        "        `attention_output` if `False`. Defaults to `False`.\n",
        "      training: Python boolean indicating whether the layer should behave in\n",
        "        training mode (adding dropout) or in inference mode (no dropout).\n",
        "        Defaults to either using the training mode of the parent layer/model,\n",
        "        or False (inference) if there is no parent layer.\n",
        "      use_causal_mask: A boolean to indicate whether to apply a causal mask to\n",
        "        prevent tokens from attending to future tokens (e.g., used in a decoder\n",
        "        Transformer).\n",
        "    Returns:\n",
        "      attention_output: The result of the computation, of shape `(B, T, E)`,\n",
        "        where `T` is for target sequence shapes and `E` is the query input last\n",
        "        dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
        "        are projected to the shape specified by `output_shape`.\n",
        "      attention_scores: [Optional] multi-head attention coefficients over\n",
        "        attention axes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        key_dim,\n",
        "        value_dim=None,\n",
        "        dropout=0.0,\n",
        "        use_bias=True,\n",
        "        output_shape=None,\n",
        "        attention_axes=None,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        k_value=32,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self._num_heads = num_heads\n",
        "        self._key_dim = key_dim\n",
        "        self._value_dim = value_dim if value_dim else key_dim\n",
        "        self._dropout = dropout\n",
        "        self._use_bias = use_bias\n",
        "        self.k_value = k_value\n",
        "        self._output_shape = output_shape\n",
        "        self._kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self._bias_initializer = initializers.get(bias_initializer)\n",
        "        self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self._bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self._activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self._kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self._bias_constraint = constraints.get(bias_constraint)\n",
        "        if attention_axes is not None and not isinstance(\n",
        "            attention_axes, collections.abc.Sized\n",
        "        ):\n",
        "            self._attention_axes = (attention_axes,)\n",
        "        else:\n",
        "            self._attention_axes = attention_axes\n",
        "        self._built_from_signature = False\n",
        "        self._query_shape, self._key_shape, self._value_shape = None, None, None\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"num_heads\": self._num_heads,\n",
        "            \"key_dim\": self._key_dim,\n",
        "            \"value_dim\": self._value_dim,\n",
        "            \"dropout\": self._dropout,\n",
        "            \"use_bias\": self._use_bias,\n",
        "            \"output_shape\": self._output_shape,\n",
        "            \"attention_axes\": self._attention_axes,\n",
        "            \"kernel_initializer\": initializers.serialize(\n",
        "                self._kernel_initializer\n",
        "            ),\n",
        "            \"bias_initializer\": initializers.serialize(self._bias_initializer),\n",
        "            \"kernel_regularizer\": regularizers.serialize(\n",
        "                self._kernel_regularizer\n",
        "            ),\n",
        "            \"bias_regularizer\": regularizers.serialize(self._bias_regularizer),\n",
        "            \"activity_regularizer\": regularizers.serialize(\n",
        "                self._activity_regularizer\n",
        "            ),\n",
        "            \"kernel_constraint\": constraints.serialize(self._kernel_constraint),\n",
        "            \"bias_constraint\": constraints.serialize(self._bias_constraint),\n",
        "            \"query_shape\": self._query_shape,\n",
        "            \"key_shape\": self._key_shape,\n",
        "            \"value_shape\": self._value_shape,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # If the layer has a different build() function from the Keras default,\n",
        "        # we need to trigger the customized build to create weights.\n",
        "        query_shape = config.pop(\"query_shape\")\n",
        "        key_shape = config.pop(\"key_shape\")\n",
        "        value_shape = config.pop(\"value_shape\")\n",
        "        layer = cls(**config)\n",
        "        if None in [query_shape, key_shape, value_shape]:\n",
        "            logging.warning(\n",
        "                \"One of dimensions of the input shape is missing. It \"\n",
        "                \"should have been memorized when the layer was serialized. \"\n",
        "                \"%s is created without weights.\",\n",
        "                str(cls),\n",
        "            )\n",
        "        else:\n",
        "            layer._build_from_signature(query_shape, value_shape, key_shape)\n",
        "        return layer\n",
        "\n",
        "    def _build_from_signature(self, query, value, key=None):\n",
        "        \"\"\"Builds layers and variables.\n",
        "        Once the method is called, self._built_from_signature will be set to\n",
        "        True.\n",
        "        Args:\n",
        "          query: Query tensor or TensorShape.\n",
        "          value: Value tensor or TensorShape.\n",
        "          key: Key tensor or TensorShape.\n",
        "        \"\"\"\n",
        "        self._built_from_signature = True\n",
        "        if hasattr(query, \"shape\"):\n",
        "            self._query_shape = tf.TensorShape(query.shape)\n",
        "        else:\n",
        "            self._query_shape = tf.TensorShape(query)\n",
        "        if hasattr(value, \"shape\"):\n",
        "            self._value_shape = tf.TensorShape(value.shape)\n",
        "        else:\n",
        "            self._value_shape = tf.TensorShape(value)\n",
        "        if key is None:\n",
        "            self._key_shape = self._value_shape\n",
        "        elif hasattr(key, \"shape\"):\n",
        "            self._key_shape = tf.TensorShape(key.shape)\n",
        "        else:\n",
        "            self._key_shape = tf.TensorShape(key)\n",
        "\n",
        "        # Any setup work performed only once should happen in an `init_scope`\n",
        "        # to avoid creating symbolic Tensors that will later pollute any eager\n",
        "        # operations.\n",
        "        with tf_utils.maybe_init_scope(self):\n",
        "            free_dims = self._query_shape.rank - 1\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                free_dims, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._query_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._key_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"query\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                self._key_shape.rank - 1, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._key_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._key_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"key\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "            self._proj=tf.keras.layers.EinsumDense(\"bjhd,kj->bkhd\",output_shape=(self.k_value,None,None))\n",
        "\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                self._value_shape.rank - 1, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._value_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._value_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"value\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "\n",
        "            # Builds the attention computations for multi-head dot product\n",
        "            # attention.  These computations could be wrapped into the keras\n",
        "            # attention layer once it supports mult-head einsum computations.\n",
        "            self._build_attention(output_rank)\n",
        "            self._output_dense = self._make_output_dense(\n",
        "                free_dims,\n",
        "                self._get_common_kwargs_for_sublayer(),\n",
        "                \"attention_output\",\n",
        "            )\n",
        "\n",
        "    def _get_common_kwargs_for_sublayer(self):\n",
        "        common_kwargs = dict(\n",
        "            kernel_regularizer=self._kernel_regularizer,\n",
        "            bias_regularizer=self._bias_regularizer,\n",
        "            activity_regularizer=self._activity_regularizer,\n",
        "            kernel_constraint=self._kernel_constraint,\n",
        "            bias_constraint=self._bias_constraint,\n",
        "        )\n",
        "        # Create new clone of kernel/bias initializer, so that we don't reuse\n",
        "        # the initializer instance, which could lead to same init value since\n",
        "        # initializer is stateless.\n",
        "        kernel_initializer = self._kernel_initializer.__class__.from_config(\n",
        "            self._kernel_initializer.get_config()\n",
        "        )\n",
        "        bias_initializer = self._bias_initializer.__class__.from_config(\n",
        "            self._bias_initializer.get_config()\n",
        "        )\n",
        "        common_kwargs[\"kernel_initializer\"] = kernel_initializer\n",
        "        common_kwargs[\"bias_initializer\"] = bias_initializer\n",
        "        return common_kwargs\n",
        "\n",
        "    def _make_output_dense(self, free_dims, common_kwargs, name=None):\n",
        "        \"\"\"Builds the output projection matrix.\n",
        "        Args:\n",
        "          free_dims: Number of free dimensions for einsum equation building.\n",
        "          common_kwargs: Common keyword arguments for einsum layer.\n",
        "          name: Name for the projection layer.\n",
        "        Returns:\n",
        "          Projection layer.\n",
        "        \"\"\"\n",
        "        if self._output_shape:\n",
        "            if not isinstance(self._output_shape, collections.abc.Sized):\n",
        "                output_shape = [self._output_shape]\n",
        "            else:\n",
        "                output_shape = self._output_shape\n",
        "        else:\n",
        "            output_shape = [self._query_shape[-1]]\n",
        "        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "            free_dims, bound_dims=2, output_dims=len(output_shape)\n",
        "        )\n",
        "        return core.EinsumDense(\n",
        "            einsum_equation,\n",
        "            output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
        "            bias_axes=bias_axes if self._use_bias else None,\n",
        "            name=name,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "\n",
        "    def _build_attention(self, rank):\n",
        "        \"\"\"Builds multi-head dot-product attention computations.\n",
        "        This function builds attributes necessary for `_compute_attention` to\n",
        "        customize attention computation to replace the default dot-product\n",
        "        attention.\n",
        "        Args:\n",
        "          rank: the rank of query, key, value tensors.\n",
        "        \"\"\"\n",
        "        if self._attention_axes is None:\n",
        "            self._attention_axes = tuple(range(1, rank - 2))\n",
        "        else:\n",
        "            self._attention_axes = tuple(self._attention_axes)\n",
        "        (\n",
        "            self._dot_product_equation,\n",
        "            self._combine_equation,\n",
        "            attn_scores_rank,\n",
        "        ) = _build_attention_equation(rank, attn_axes=self._attention_axes)\n",
        "        norm_axes = tuple(\n",
        "            range(\n",
        "                attn_scores_rank - len(self._attention_axes), attn_scores_rank\n",
        "            )\n",
        "        )\n",
        "        self._softmax = activation.Softmax(axis=norm_axes)\n",
        "        self._dropout_layer = regularization.Dropout(rate=self._dropout)\n",
        "\n",
        "    def _masked_softmax(self, attention_scores, attention_mask=None):\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        # `attention_scores` = [B, N, T, S]\n",
        "        if attention_mask is not None:\n",
        "            # The expand dim happens starting from the `num_heads` dimension,\n",
        "            # (<batch_dims>, num_heads, <query_attention_dims,\n",
        "            # key_attention_dims>)\n",
        "            mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n",
        "            for _ in range(\n",
        "                len(attention_scores.shape) - len(attention_mask.shape)\n",
        "            ):\n",
        "                attention_mask = tf.expand_dims(\n",
        "                    attention_mask, axis=mask_expansion_axis\n",
        "                )\n",
        "        return self._softmax(attention_scores, attention_mask)\n",
        "\n",
        "    def _compute_attention(\n",
        "        self, query, key, value, attention_mask=None, training=None\n",
        "    ):\n",
        "        \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
        "        This function defines the computation inside `call` with projected\n",
        "        multi-head Q, K, V inputs. Users can override this function for\n",
        "        customized attention implementation.\n",
        "        Args:\n",
        "          query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          key: Projected key `Tensor` of shape `(B, S, N, key_dim)`.\n",
        "          value: Projected value `Tensor` of shape `(B, S, N, value_dim)`.\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions. It is generally not needed if the\n",
        "            `query` and `value` (and/or `key`) are masked.\n",
        "          training: Python boolean indicating whether the layer should behave in\n",
        "            training mode (adding dropout) or in inference mode (doing nothing).\n",
        "        Returns:\n",
        "          attention_output: Multi-headed outputs of attention computation.\n",
        "          attention_scores: Multi-headed attention weights.\n",
        "        \"\"\"\n",
        "        # Note: Applying scalar multiply at the smaller end of einsum improves\n",
        "        # XLA performance, but may introduce slight numeric differences in\n",
        "        # the Transformer attention head.\n",
        "        query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "        # attention scores.\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(\n",
        "            attention_scores, attention_mask\n",
        "        )\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "\n",
        "        # `context_layer` = [B, T, N, H]\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        #import sys\n",
        "        #sys.exit()\n",
        "        return attention_output, attention_scores\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        query,\n",
        "        value,\n",
        "        key=None,\n",
        "        attention_mask=None,\n",
        "        return_attention_scores=False,\n",
        "        training=None,\n",
        "        use_causal_mask=False,\n",
        "    ):\n",
        "        attention_mask = self._compute_attention_mask(\n",
        "            query,\n",
        "            value,\n",
        "            key=key,\n",
        "            attention_mask=attention_mask,\n",
        "            use_causal_mask=use_causal_mask,\n",
        "        )\n",
        "\n",
        "        if not self._built_from_signature:\n",
        "            self._build_from_signature(query=query, value=value, key=key)\n",
        "        if key is None:\n",
        "            key = value\n",
        "\n",
        "        query_is_ragged = isinstance(query, tf.RaggedTensor)\n",
        "        if query_is_ragged:\n",
        "            query_lengths = query.nested_row_lengths()\n",
        "            query = query.to_tensor()\n",
        "\n",
        "        key_is_ragged = isinstance(key, tf.RaggedTensor)\n",
        "        value_is_ragged = isinstance(value, tf.RaggedTensor)\n",
        "        if key_is_ragged and value_is_ragged:\n",
        "            # Ensure they have the same shape.\n",
        "            bounding_shape = tf.math.maximum(\n",
        "                key.bounding_shape(), value.bounding_shape()\n",
        "            )\n",
        "            key = key.to_tensor(shape=bounding_shape)\n",
        "            value = value.to_tensor(shape=bounding_shape)\n",
        "        elif key_is_ragged:\n",
        "            key = key.to_tensor(shape=tf.shape(value))\n",
        "        elif value_is_ragged:\n",
        "            value = value.to_tensor(shape=tf.shape(key))\n",
        "\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        # `query` = [B, T, N ,H]\n",
        "        query = self._query_dense(query)\n",
        "        # `key` = [B, S, N, H]\n",
        "        key = self._key_dense(key)\n",
        "        key = self._proj(key)\n",
        "        # `value` = [B, S, N, H]\n",
        "        value = self._value_dense(value)\n",
        "        value = self._proj(value)\n",
        "        attention_output, attention_scores = self._compute_attention(\n",
        "            query, key, value, attention_mask, training\n",
        "        )\n",
        "        attention_output = self._output_dense(attention_output)\n",
        "\n",
        "        if query_is_ragged:\n",
        "            attention_output = tf.RaggedTensor.from_tensor(\n",
        "                attention_output, lengths=query_lengths\n",
        "            )\n",
        "\n",
        "        if return_attention_scores:\n",
        "            return attention_output, attention_scores\n",
        "        return attention_output\n",
        "\n",
        "    def _compute_attention_mask(\n",
        "        self, query, value, key=None, attention_mask=None, use_causal_mask=False\n",
        "    ):\n",
        "        \"\"\"Computes the attention mask, using the Keras masks of the inputs.\n",
        "        * The `query`'s mask is reshaped from [B, T] to [B, T, 1].\n",
        "        * The `value`'s mask is reshaped from [B, S] to [B, 1, S].\n",
        "        * The `key`'s mask is reshaped from [B, S] to [B, 1, S]. The `key`'s\n",
        "          mask is ignored if `key` is `None` or if `key is value`.\n",
        "        * If `use_causal_mask=True`, then the causal mask is computed. Its shape\n",
        "          is [1, T, S].\n",
        "        All defined masks are merged using a logical AND operation (`&`).\n",
        "        In general, if the `query` and `value` are masked, then there is no need\n",
        "        to define the `attention_mask`.\n",
        "        Args:\n",
        "          query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions.\n",
        "          use_causal_mask: A boolean to indicate whether to apply a causal mask\n",
        "            to prevent tokens from attending to future tokens (e.g., used in a\n",
        "            decoder Transformer).\n",
        "        Returns:\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions, based on the Keras masks of the\n",
        "            `query`, `key`, `value`, and `attention_mask` tensors, and the\n",
        "            causal mask if `use_causal_mask=True`.\n",
        "        \"\"\"\n",
        "        query_mask = getattr(query, \"_keras_mask\", None)\n",
        "        value_mask = getattr(value, \"_keras_mask\", None)\n",
        "        key_mask = getattr(key, \"_keras_mask\", None)\n",
        "        auto_mask = None\n",
        "        if query_mask is not None:\n",
        "            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, T = max query length\n",
        "            auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, T, 1]\n",
        "        if value_mask is not None:\n",
        "            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, S == max value length\n",
        "            mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if key_mask is not None:\n",
        "            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n",
        "            # B == batch size, S == max key length == max value length\n",
        "            mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if use_causal_mask:\n",
        "            # the shape of the causal mask is [1, T, S]\n",
        "            mask = self._compute_causal_mask(query, value)\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if auto_mask is not None:\n",
        "            # merge attention_mask & automatic mask, to shape [B, T, S]\n",
        "            attention_mask = (\n",
        "                auto_mask\n",
        "                if attention_mask is None\n",
        "                else tf.cast(attention_mask, bool) & auto_mask\n",
        "            )\n",
        "        return attention_mask\n",
        "\n",
        "    def _compute_causal_mask(self, query, value=None):\n",
        "        \"\"\"Computes a causal mask (e.g., for masked self-attention layers).\n",
        "        For example, if query and value both contain sequences of length 4,\n",
        "        this function returns a boolean `Tensor` equal to:\n",
        "        ```\n",
        "        [[[True,  False, False, False],\n",
        "          [True,  True,  False, False],\n",
        "          [True,  True,  True,  False],\n",
        "          [True,  True,  True,  True]]]\n",
        "        ```\n",
        "        Args:\n",
        "          query: query `Tensor` of shape `(B, T, ...)`.\n",
        "          value: value `Tensor` of shape `(B, S, ...)` (optional, defaults to\n",
        "          query).\n",
        "        Returns:\n",
        "          mask: a boolean `Tensor` of shape [1, T, S] containing a lower\n",
        "                triangular matrix of shape [T, S].\n",
        "        \"\"\"\n",
        "        q_seq_length = tf.shape(query)[1]\n",
        "        v_seq_length = q_seq_length if value is None else tf.shape(value)[1]\n",
        "        return tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "            tf.ones((1, q_seq_length, v_seq_length), tf.bool), -1, 0\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, query_shape, value_shape, key_shape=None):\n",
        "\n",
        "        if key_shape is None:\n",
        "            key_shape = value_shape\n",
        "\n",
        "        query_shape = tf.TensorShape(query_shape)\n",
        "        value_shape = tf.TensorShape(value_shape)\n",
        "        key_shape = tf.TensorShape(key_shape)\n",
        "\n",
        "        if query_shape[-1] != value_shape[-1]:\n",
        "            raise ValueError(\n",
        "                \"The last dimension of `query_shape` and `value_shape` \"\n",
        "                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n",
        "                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n",
        "            )\n",
        "\n",
        "        if value_shape[1:-1] != key_shape[1:-1]:\n",
        "            raise ValueError(\n",
        "                \"All dimensions of `value` and `key`, except the last one, \"\n",
        "                f\"must be equal. Received {value_shape} and \"\n",
        "                f\"{key_shape}\"\n",
        "            )\n",
        "\n",
        "        if self._output_shape:\n",
        "            return query_shape[:-1].concatenate(self._output_shape)\n",
        "\n",
        "        return query_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSH Attention as keras multihead subclass"
      ],
      "metadata": {
        "id": "yuU1I__9PLzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers.attention.multi_head_attention import MultiHeadAttention\n",
        "from keras.utils import tf_utils\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSHAttention(MultiHeadAttention):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    self.n_buckets= kwargs.pop('n_buckets', 4)\n",
        "    super().__init__(*args, **kwargs)\n",
        "  def _build_from_signature(self, query, value, key=None):\n",
        "    with tf_utils.maybe_init_scope(self):\n",
        "      self._hash=tf.keras.layers.EinsumDense(\"bjhd,dc->bjhc\",output_shape=(None,None,self.n_buckets // 2))\n",
        "      self.lsh_proj=tf.random.normal\n",
        "    super()._build_from_signature(query, value, key=None)\n",
        "\n",
        "  def call(\n",
        "        self,\n",
        "        query,\n",
        "        value,\n",
        "        key=None,\n",
        "        attention_mask=None,\n",
        "        return_attention_scores=False,\n",
        "        training=None,\n",
        "        use_causal_mask=False,\n",
        "        ):\n",
        "        attention_mask = self._compute_attention_mask(\n",
        "            query,\n",
        "            value,\n",
        "            key=key,\n",
        "            attention_mask=attention_mask,\n",
        "            use_causal_mask=use_causal_mask,\n",
        "        )\n",
        "\n",
        "        if not self._built_from_signature:\n",
        "            self._build_from_signature(query=query, value=value, key=key)\n",
        "        if key is None:\n",
        "            key = value\n",
        "\n",
        "        query_is_ragged = isinstance(query, tf.RaggedTensor)\n",
        "        if query_is_ragged:\n",
        "            query_lengths = query.nested_row_lengths()\n",
        "            query = query.to_tensor()\n",
        "\n",
        "        key_is_ragged = isinstance(key, tf.RaggedTensor)\n",
        "        value_is_ragged = isinstance(value, tf.RaggedTensor)\n",
        "        if key_is_ragged and value_is_ragged:\n",
        "            # Ensure they have the same shape.\n",
        "            bounding_shape = tf.math.maximum(\n",
        "                key.bounding_shape(), value.bounding_shape()\n",
        "            )\n",
        "            key = key.to_tensor(shape=bounding_shape)\n",
        "            value = value.to_tensor(shape=bounding_shape)\n",
        "        elif key_is_ragged:\n",
        "            key = key.to_tensor(shape=tf.shape(value))\n",
        "        elif value_is_ragged:\n",
        "            value = value.to_tensor(shape=tf.shape(key))\n",
        "\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        # `query` = [B, T, N ,H]\n",
        "        query = self._query_dense(query)\n",
        "        # `key` = [B, S, N, H]\n",
        "        key = query\n",
        "        # `value` = [B, S, N, H]\n",
        "        value = self._value_dense(value)\n",
        "        for i in range(value.shape[2]):\n",
        "          #here we make the rotation matrix, that will hash the vectors\n",
        "          random_rot=self.lsh_proj([value.shape[-1],self.n_buckets // 2])\n",
        "          #hash=self._hash(value)\n",
        "\n",
        "          #here we make the rotation using dot product, and making the buckets\n",
        "          hash=tf.matmul(value[:,:,i,:],random_rot)\n",
        "          hash=tf.concat([hash,-hash],axis=-1)\n",
        "\n",
        "          #here we make each token be clasified to a bucket\n",
        "          buckets=tf.math.argmax(hash,axis=-1)\n",
        "\n",
        "          #This is the index that will sort the inputs, and is made by sorting the buckets\n",
        "          index=tf.argsort(buckets,axis=-1)\n",
        "\n",
        "          #here we sort the main inputs usinf the index\n",
        "          sv=tf.gather(value[:,:,i,:],index,axis=1,batch_dims=-1)\n",
        "          sq=sk=tf.gather(query[:,:,i,:],index,axis=1,batch_dims=-1)\n",
        "          #And chunk the inputs based on how many buckets we have\n",
        "          sv=tf.split(sv, self.n_buckets, axis=1)\n",
        "          sq=tf.split(sq, self.n_buckets, axis=1)\n",
        "          j=0\n",
        "          for v,q in zip(sv,sq):\n",
        "            attention_output, attention_scores = self._compute_attention(\n",
        "                q[:,:,None,:], q[:,:,None,:], v[:,:,None,:], attention_mask, training\n",
        "            )\n",
        "            if j==0:\n",
        "              att=attention_output\n",
        "            else:\n",
        "              att=tf.concat([att,attention_output],axis=1)\n",
        "            j+=1\n",
        "          attention_output=att\n",
        "          if i==0:\n",
        "            att_o=attention_output\n",
        "          else:\n",
        "            att_o=tf.concat([att_o,attention_output],axis=2)\n",
        "        attention_output = att_o\n",
        "        index=tf.argsort(index,axis=-1)\n",
        "        attention_output = tf.gather(attention_output, index,axis=1,batch_dims=-1)\n",
        "        attention_output = self._output_dense(attention_output)\n",
        "        if query_is_ragged:\n",
        "            attention_output = tf.RaggedTensor.from_tensor(\n",
        "                attention_output, lengths=query_lengths\n",
        "        )\n",
        "        if return_attention_scores:\n",
        "            return attention_output, attention_scores\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "UhMT80KNPLCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers.attention.multi_head_attention import MultiHeadAttention\n",
        "from keras.utils import tf_utils\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LSHAttention(MultiHeadAttention):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    self.n_buckets= kwargs.pop('n_buckets', 4)\n",
        "    super().__init__(*args, **kwargs)\n",
        "  def _build_from_signature(self, query, value, key=None):\n",
        "    with tf_utils.maybe_init_scope(self):\n",
        "      self.lsh_proj=tf.random.normal\n",
        "    super()._build_from_signature(query, value, key=None)\n",
        "\n",
        "  def call(\n",
        "        self,\n",
        "        query,\n",
        "        value,\n",
        "        key=None,\n",
        "        attention_mask=None,\n",
        "        return_attention_scores=False,\n",
        "        training=None,\n",
        "        use_causal_mask=False,\n",
        "        ):\n",
        "        attention_mask = self._compute_attention_mask(\n",
        "            query,\n",
        "            value,\n",
        "            key=key,\n",
        "            attention_mask=attention_mask,\n",
        "            use_causal_mask=use_causal_mask,\n",
        "        )\n",
        "\n",
        "        if not self._built_from_signature:\n",
        "            self._build_from_signature(query=query, value=value, key=key)\n",
        "        if key is None:\n",
        "            key = value\n",
        "\n",
        "        query_is_ragged = isinstance(query, tf.RaggedTensor)\n",
        "        if query_is_ragged:\n",
        "            query_lengths = query.nested_row_lengths()\n",
        "            query = query.to_tensor()\n",
        "\n",
        "        key_is_ragged = isinstance(key, tf.RaggedTensor)\n",
        "        value_is_ragged = isinstance(value, tf.RaggedTensor)\n",
        "        if key_is_ragged and value_is_ragged:\n",
        "            # Ensure they have the same shape.\n",
        "            bounding_shape = tf.math.maximum(\n",
        "                key.bounding_shape(), value.bounding_shape()\n",
        "            )\n",
        "            key = key.to_tensor(shape=bounding_shape)\n",
        "            value = value.to_tensor(shape=bounding_shape)\n",
        "        elif key_is_ragged:\n",
        "            key = key.to_tensor(shape=tf.shape(value))\n",
        "        elif value_is_ragged:\n",
        "            value = value.to_tensor(shape=tf.shape(key))\n",
        "\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        # `query` = [B, T, N ,H]\n",
        "        query = self._query_dense(query)\n",
        "        # `key` = [B, S, N, H]\n",
        "        key = query\n",
        "        # `value` = [B, S, N, H]\n",
        "        value = self._value_dense(value)\n",
        "        for i in range(value.shape[2]):\n",
        "          #here we make the rotation matrix, that will hash the vectors\n",
        "          random_rot=self.lsh_proj([value.shape[-1],self.n_buckets // 2])\n",
        "          #hash=self._hash(value)\n",
        "\n",
        "          #here we make the rotation using dot product, and making the buckets\n",
        "          hash=tf.matmul(value[:,:,i,:],random_rot)\n",
        "          hash=tf.concat([hash,-hash],axis=-1)\n",
        "\n",
        "          #here we make each token be clasified to a bucket\n",
        "          buckets=tf.math.argmax(hash,axis=-1)\n",
        "\n",
        "          #This is the index that will sort the inputs, and is made by sorting the buckets\n",
        "          index=tf.argsort(buckets,axis=-1)\n",
        "\n",
        "          #here we sort the main inputs usinf the index\n",
        "          sv=tf.gather(value[:,:,i,:],index,axis=1,batch_dims=-1)\n",
        "          sq=sk=tf.gather(query[:,:,i,:],index,axis=1,batch_dims=-1)\n",
        "          #And chunk the inputs based on how many buckets we have\n",
        "\n",
        "          j=0\n",
        "          chunk_size=int(sv.shape[1]/self.n_buckets)\n",
        "          for j in range(self.n_buckets):\n",
        "            q=sq[:,j*chunk_size:(j+1)*chunk_size,:]\n",
        "            v=sv[:,j*chunk_size:(j+1)*chunk_size,:]\n",
        "            attention_output, attention_scores = self._compute_attention(\n",
        "                q[:,:,None,:], q[:,:,None,:], v[:,:,None,:], attention_mask, training\n",
        "            )\n",
        "            if j==0:\n",
        "              att=attention_output\n",
        "            else:\n",
        "              att=tf.concat([att,attention_output],axis=1)\n",
        "          attention_output=att\n",
        "          if i==0:\n",
        "            att_o=attention_output\n",
        "          else:\n",
        "            att_o=tf.concat([att_o,attention_output],axis=2)\n",
        "        attention_output = att_o\n",
        "        attention_output = self._output_dense(attention_output)\n",
        "        if query_is_ragged:\n",
        "            attention_output = tf.RaggedTensor.from_tensor(\n",
        "                attention_output, lengths=query_lengths\n",
        "        )\n",
        "        if return_attention_scores:\n",
        "            return attention_output, attention_scores\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "MqxDD_D1mJGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKms-Jo8H-kn"
      },
      "source": [
        "##Patch wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUySwAEoH-ko"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W65NvnvZH-kp"
      },
      "outputs": [],
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6\n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kli_5DCBH-kp"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def iou_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
        "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "  return iou\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
        "  return dice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      #None,128,128,1\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      #None,128,128\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += dice_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ],
      "metadata": {
        "id": "LmLKTdyUH-kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += iou_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ],
      "metadata": {
        "id": "oopp7EhGH-kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cih6HuiDH-kr"
      },
      "outputs": [],
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (3, 3), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (1, 1), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "metadata": {
        "id": "zx_Lky4g-HLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Flatten(M):\n",
        "  s1=M.shape[1]\n",
        "  s2=M.shape[2]\n",
        "  s3=M.shape[3]\n",
        "  flat=s1*s2\n",
        "  M= tf.reshape(M,[-1,flat,s3])\n",
        "  return M,s1,s2,s3"
      ],
      "metadata": {
        "id": "criSasUmEUSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Flatten_LSH(M,N,nheads,dimk,k=128):\n",
        "  M,s1,s2,s3= Flatten(M)\n",
        "  N,_,_,_= Flatten(N)\n",
        "  pm=LSHAttention(num_heads=nheads,key_dim=dimk,dropout=0.2,n_buckets=k)(M,M)\n",
        "  pm=tf.reshape(pm,[-1,s1,s2,s3])\n",
        "  return pm"
      ],
      "metadata": {
        "id": "0NfKBfC9D1CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaBbrDNpD1CL"
      },
      "outputs": [],
      "source": [
        "def vit(X,Y,n_heads=16,psize=16,k=256,dim=\"4D\"):\n",
        "  npatch=int(X.shape[1]/psize)\n",
        "  psizey=int(psize/2)\n",
        "  kdim=int(X.shape[3]/n_heads)\n",
        "  Y=tf.keras.layers.Dense(X.shape[3])(Y)\n",
        "  if npatch==0:\n",
        "    pm=Flatten_LSH(X,Y,n_heads,kdim,k)\n",
        "    return pm\n",
        "  for i in range(npatch):\n",
        "    st=i*psize\n",
        "    sty=i*psizey\n",
        "    end=st+psize\n",
        "    endy=sty+psizey\n",
        "    for j in range(npatch):\n",
        "      st2=j*psize\n",
        "      end2=st2+psize\n",
        "      sty2=j*psizey\n",
        "      endy2=sty2+psizey\n",
        "      patch= X[:,st:end,st2:end2,:]\n",
        "      patch2= Y[:,sty:endy,sty2:endy2,:]\n",
        "      pm=Flatten_LSH(patch,patch2,n_heads,kdim,k)\n",
        "      if j==0:\n",
        "        A=pm\n",
        "      else:\n",
        "        A=tf.concat([A,pm],2)\n",
        "    if i==0:\n",
        "      V=A\n",
        "    else:\n",
        "      V=tf.concat([V,A],1)\n",
        "  return V\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXKkrPRH-kx"
      },
      "source": [
        "### Vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h9vieygH-kx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  nheads=4\n",
        "  sizep=32\n",
        "  gr=4\n",
        "  kval=4\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=gr)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=gr)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=gr)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5,z5 = conv_block(p4,[256,256],0.3,group=gr)\n",
        "\n",
        "  #Expansive path\n",
        "  m1= vit(z4,z5,nheads,sizep,k=kval)\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2,group=gr)\n",
        "\n",
        "  m2= vit(z3,z6,nheads,sizep,k=kval)\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2,group=gr)\n",
        "\n",
        "  m3= vit(z2,z7,nheads,sizep,k=kval)\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32],group=gr)\n",
        "\n",
        "  m4= vit(z1,z8,nheads,sizep,k=kval)\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=gr)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_coef_multilabel,dice_coef_multilabel])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSH without Patches"
      ],
      "metadata": {
        "id": "vUN1FbBxpx23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Flatten(M):\n",
        "  s1=M.shape[1]\n",
        "  s2=M.shape[2]\n",
        "  s3=M.shape[3]\n",
        "  flat=s1*s2\n",
        "  M= tf.reshape(M,[-1,flat,s3])\n",
        "  return M,s1,s2,s3"
      ],
      "metadata": {
        "id": "AOWw_y4D4cMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSH_lay(M,nheads,k=128,nbuckets=4):\n",
        "  M,s1,s2,s3= Flatten(M)\n",
        "  dimk=int(M.shape[2]/nheads)\n",
        "  pm=LSHAttention(num_heads=nheads,key_dim=dimk,dropout=0.2,n_buckets=nbuckets)(M,M)\n",
        "  pm=tf.reshape(pm,[-1,s1,s2,s3])\n",
        "  return pm"
      ],
      "metadata": {
        "id": "qi15V9Ihp8SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-3KKmhCp4Ue"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  nheads=4\n",
        "  sizep=32\n",
        "  gr=4\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=gr)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=gr)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=gr)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5,z5 = conv_block(p4,[256,256],0.3,group=gr)\n",
        "\n",
        "  #Expansive path\n",
        "\n",
        "  m1= LSH_lay(z4,nheads)\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2,group=gr)\n",
        "\n",
        "  m2= LSH_lay(z3,nheads,nbuckets=8)\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2,group=gr)\n",
        "\n",
        "  m3= LSH_lay(z2,nheads,nbuckets=8)\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32],group=gr)\n",
        "\n",
        "  m4= LSH_lay(z1,nheads,nbuckets=32)\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=gr)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_coef_multilabel,dice_coef_multilabel])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "HlE3bWkIpvDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_model()\n",
        "model.fit(X_train,Y_train,batch_size=16,epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UGu9XXsJmbg",
        "outputId": "66f27497-c3a0-44e2-dda7-971c47b49647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "158/158 [==============================] - 454s 875ms/step - loss: 0.2368 - accuracy: 0.9364 - iou_coef_multilabel: 0.3757 - dice_coef_multilabel: 0.4452\n",
            "Epoch 2/25\n",
            "158/158 [==============================] - 127s 806ms/step - loss: 0.0523 - accuracy: 0.9863 - iou_coef_multilabel: 0.5403 - dice_coef_multilabel: 0.5984\n",
            "Epoch 3/25\n",
            "158/158 [==============================] - 127s 803ms/step - loss: 0.0356 - accuracy: 0.9898 - iou_coef_multilabel: 0.5898 - dice_coef_multilabel: 0.6427\n",
            "Epoch 4/25\n",
            "158/158 [==============================] - 127s 807ms/step - loss: 0.0286 - accuracy: 0.9913 - iou_coef_multilabel: 0.6189 - dice_coef_multilabel: 0.6692\n",
            "Epoch 5/25\n",
            "158/158 [==============================] - 127s 802ms/step - loss: 0.0245 - accuracy: 0.9922 - iou_coef_multilabel: 0.6402 - dice_coef_multilabel: 0.6887\n",
            "Epoch 6/25\n",
            "158/158 [==============================] - 127s 803ms/step - loss: 0.0221 - accuracy: 0.9928 - iou_coef_multilabel: 0.6557 - dice_coef_multilabel: 0.7027\n",
            "Epoch 7/25\n",
            "158/158 [==============================] - 127s 802ms/step - loss: 0.0208 - accuracy: 0.9931 - iou_coef_multilabel: 0.6670 - dice_coef_multilabel: 0.7131\n",
            "Epoch 8/25\n",
            "158/158 [==============================] - 127s 802ms/step - loss: 0.0193 - accuracy: 0.9935 - iou_coef_multilabel: 0.6788 - dice_coef_multilabel: 0.7238\n",
            "Epoch 9/25\n",
            "158/158 [==============================] - 127s 803ms/step - loss: 0.0184 - accuracy: 0.9936 - iou_coef_multilabel: 0.6873 - dice_coef_multilabel: 0.7316\n",
            "Epoch 10/25\n",
            "158/158 [==============================] - 127s 802ms/step - loss: 0.0177 - accuracy: 0.9938 - iou_coef_multilabel: 0.6965 - dice_coef_multilabel: 0.7401\n",
            "Epoch 11/25\n",
            "158/158 [==============================] - 127s 801ms/step - loss: 0.0170 - accuracy: 0.9940 - iou_coef_multilabel: 0.7044 - dice_coef_multilabel: 0.7475\n",
            "Epoch 12/25\n",
            "158/158 [==============================] - 126s 799ms/step - loss: 0.0165 - accuracy: 0.9941 - iou_coef_multilabel: 0.7121 - dice_coef_multilabel: 0.7546\n",
            "Epoch 13/25\n",
            "158/158 [==============================] - 126s 800ms/step - loss: 0.0160 - accuracy: 0.9943 - iou_coef_multilabel: 0.7199 - dice_coef_multilabel: 0.7620\n",
            "Epoch 14/25\n",
            "158/158 [==============================] - 127s 801ms/step - loss: 0.0155 - accuracy: 0.9944 - iou_coef_multilabel: 0.7268 - dice_coef_multilabel: 0.7684\n",
            "Epoch 15/25\n",
            "158/158 [==============================] - 126s 800ms/step - loss: 0.0153 - accuracy: 0.9945 - iou_coef_multilabel: 0.7330 - dice_coef_multilabel: 0.7744\n",
            "Epoch 16/25\n",
            "158/158 [==============================] - 126s 799ms/step - loss: 0.0149 - accuracy: 0.9946 - iou_coef_multilabel: 0.7385 - dice_coef_multilabel: 0.7794\n",
            "Epoch 17/25\n",
            "158/158 [==============================] - 126s 795ms/step - loss: 0.0147 - accuracy: 0.9946 - iou_coef_multilabel: 0.7456 - dice_coef_multilabel: 0.7863\n",
            "Epoch 18/25\n",
            "158/158 [==============================] - 126s 799ms/step - loss: 0.0144 - accuracy: 0.9947 - iou_coef_multilabel: 0.7503 - dice_coef_multilabel: 0.7908\n",
            "Epoch 19/25\n",
            "158/158 [==============================] - 126s 799ms/step - loss: 0.0141 - accuracy: 0.9948 - iou_coef_multilabel: 0.7579 - dice_coef_multilabel: 0.7981\n",
            "Epoch 20/25\n",
            "158/158 [==============================] - 126s 797ms/step - loss: 0.0142 - accuracy: 0.9947 - iou_coef_multilabel: 0.7562 - dice_coef_multilabel: 0.7964\n",
            "Epoch 21/25\n",
            "158/158 [==============================] - 126s 800ms/step - loss: 0.0141 - accuracy: 0.9947 - iou_coef_multilabel: 0.7630 - dice_coef_multilabel: 0.8030\n",
            "Epoch 22/25\n",
            "158/158 [==============================] - 126s 799ms/step - loss: 0.0136 - accuracy: 0.9949 - iou_coef_multilabel: 0.7713 - dice_coef_multilabel: 0.8110\n",
            "Epoch 23/25\n",
            "158/158 [==============================] - 126s 797ms/step - loss: 0.0134 - accuracy: 0.9950 - iou_coef_multilabel: 0.7764 - dice_coef_multilabel: 0.8157\n",
            "Epoch 24/25\n",
            "158/158 [==============================] - 126s 798ms/step - loss: 0.0132 - accuracy: 0.9950 - iou_coef_multilabel: 0.7804 - dice_coef_multilabel: 0.8196\n",
            "Epoch 25/25\n",
            "158/158 [==============================] - 126s 797ms/step - loss: 0.0131 - accuracy: 0.9950 - iou_coef_multilabel: 0.7846 - dice_coef_multilabel: 0.8234\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff23d9adab0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for split version, time for execution 42 m\n",
        "\n",
        "patched version time for execution 34 m"
      ],
      "metadata": {
        "id": "UBnt5OP2nQEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yNkgZfLmnD8",
        "outputId": "220dfc67-5397-4d16-cdb9-3e983cf99477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.023588653653860092; accuracy of 99.25824999809265% iou_coef_multilabel of 75.87804794311523% dice_coef_multilabel of 79.55738306045532%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patch size=32 k=32:\n",
        "\n",
        "Score for fold 0: loss of 0.02660856954753399; accuracy of 99.21053647994995% iou_coef_multilabel of 73.97453188896179% dice_coef_multilabel of 77.59955525398254%\n",
        "\n",
        "No Patch k=128:\n",
        "\n",
        "Score for fold 0: loss of 0.03675605729222298; accuracy of 98.8860547542572% iou_coef_multilabel of 69.78949904441833% dice_coef_multilabel of 74.2724359035492%\n",
        "\n",
        "Patch size=32 k=256:\n",
        "Score for fold 0: loss of 0.11757280677556992; accuracy of 96.79576754570007% iou_coef_multilabel of 71.6090202331543% dice_coef_multilabel of 79.81376647949219%\n"
      ],
      "metadata": {
        "id": "OQ_noHHMsOEk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2D-muOYe_Pj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trswxiGSe_Pl"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Patch test"
      ],
      "metadata": {
        "id": "qWRqddndzFCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=32"
      ],
      "metadata": {
        "id": "4T-tvQ42fpKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUsIi9Se4PD",
        "outputId": "1f0ee790-e664-4eee-c81d-e35012365ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.029409149661660194; accuracy of 98.91234040260315% iou_coef_multilabel of 70.78478932380676% dice_coef_multilabel of 75.16862750053406%\n",
            "Score for fold 2: loss of 0.042917650192976; accuracy of 98.51773381233215% iou_coef_multilabel of 70.23463249206543% dice_coef_multilabel of 75.7128119468689%\n",
            "Score for fold 3: loss of 0.04444853588938713; accuracy of 98.51000308990479% iou_coef_multilabel of 67.47883558273315% dice_coef_multilabel of 72.66407012939453%\n",
            "Score for fold 4: loss of 0.10042478889226913; accuracy of 96.02788090705872% iou_coef_multilabel of 55.91818690299988% dice_coef_multilabel of 63.355553150177%\n",
            "Score for fold 5: loss of 0.03272783383727074; accuracy of 98.82147312164307% iou_coef_multilabel of 69.75324749946594% dice_coef_multilabel of 74.68957304954529%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u-iwkU7t4ak",
        "outputId": "0c504f0d-d2d5-4752-d341-f7a58c773d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.025224272161722183; accuracy of 99.0780770778656% iou_coef_multilabel of 75.7142961025238% dice_coef_multilabel of 79.58106994628906%\n",
            "Score for fold 2: loss of 0.04713413491845131; accuracy of 98.5433280467987% iou_coef_multilabel of 74.99583959579468% dice_coef_multilabel of 80.19270896911621%\n",
            "Score for fold 3: loss of 0.04239453375339508; accuracy of 98.56003522872925% iou_coef_multilabel of 71.22631072998047% dice_coef_multilabel of 76.35141015052795%\n",
            "Score for fold 4: loss of 0.04546702280640602; accuracy of 98.31485748291016% iou_coef_multilabel of 70.64926028251648% dice_coef_multilabel of 77.11712121963501%\n",
            "Score for fold 5: loss of 0.032865818589925766; accuracy of 98.83249998092651% iou_coef_multilabel of 71.10702991485596% dice_coef_multilabel of 75.85053443908691%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=256"
      ],
      "metadata": {
        "id": "Gp5y4WBGfweq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328b22fc-64d7-4970-87da-2b9867e7d106",
        "id": "2kmNTemGfvkE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.016601325944066048; accuracy of 99.49246644973755% iou_coef_multilabel of 80.1440417766571% dice_coef_multilabel of 82.72783160209656%\n",
            "Score for fold 2: loss of 0.02792888507246971; accuracy of 99.11463856697083% iou_coef_multilabel of 77.49691009521484% dice_coef_multilabel of 81.86010122299194%\n",
            "Score for fold 3: loss of 0.033090461045503616; accuracy of 99.0357756614685% iou_coef_multilabel of 75.54389238357544% dice_coef_multilabel of 79.85571622848511%\n",
            "Score for fold 4: loss of 0.02735254541039467; accuracy of 99.02375340461731% iou_coef_multilabel of 75.38833618164062% dice_coef_multilabel of 80.59991002082825%\n",
            "Score for fold 5: loss of 0.02401789091527462; accuracy of 99.26612973213196% iou_coef_multilabel of 76.66497826576233% dice_coef_multilabel of 80.13312816619873%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "ee163453-4cde-4531-bfef-16e685603493",
        "id": "i2HqP5AsfvkH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.01717947982251644; accuracy of 99.4581937789917% iou_coef_multilabel of 79.65168952941895% dice_coef_multilabel of 82.61964917182922%\n",
            "Score for fold 2: loss of 0.03299105167388916; accuracy of 99.06346797943115% iou_coef_multilabel of 78.3797025680542% dice_coef_multilabel of 82.73652791976929%\n",
            "Score for fold 3: loss of 0.034237224608659744; accuracy of 99.15516972541809% iou_coef_multilabel of 78.44327688217163% dice_coef_multilabel of 82.67260193824768%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-09b93a07fbe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####No Patch test"
      ],
      "metadata": {
        "id": "dbVD7hu9zIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60653fbf-aeeb-451c-dcdc-c37c1b1a6ccf",
        "id": "kOYSXDX_zOMg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.02731938473880291; accuracy of 99.00254607200623% iou_coef_multilabel of 71.55993580818176% dice_coef_multilabel of 75.51640272140503%\n",
            "Score for fold 2: loss of 0.04416176304221153; accuracy of 98.46460819244385% iou_coef_multilabel of 70.76845169067383% dice_coef_multilabel of 76.32638216018677%\n",
            "Score for fold 3: loss of 0.04237723350524902; accuracy of 98.45744967460632% iou_coef_multilabel of 67.4326241016388% dice_coef_multilabel of 72.71857857704163%\n",
            "Score for fold 4: loss of 0.04793655872344971; accuracy of 98.23458194732666% iou_coef_multilabel of 68.04465651512146% dice_coef_multilabel of 74.47894811630249%\n",
            "Score for fold 5: loss of 0.0348425917327404; accuracy of 98.7302839756012% iou_coef_multilabel of 65.84467887878418% dice_coef_multilabel of 70.64203023910522%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "9c7dd46a-f675-468d-8126-6fabf5e3bbbc",
        "id": "QTKk58bXzOMi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-09b93a07fbe5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9-gnhHg7mVW"
      },
      "source": [
        "##Model data augmented kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocj4dk1_7uDf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifq3F_K-7mVc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOTJFgoH7mVc"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRrgKSAz7mVc"
      },
      "outputs": [],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((X_fold, Y_fold))\n",
        "  # Apply the data augmentation pipeline to both X_train and Y_train\n",
        "  augmented_train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x, y: x)\n",
        "  Y_train_augmented = augmented_train_data.map(lambda x, y: y)\n",
        "\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = np.asarray(list(Y_train_augmented.as_numpy_iterator()))\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr59x7qJ7mVc",
        "outputId": "8626b608-fbd4-4c80-c009-a8d7f5ab8795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.06248067319393158; accuracy of 97.74013757705688% iou_coef_multilabel of 69.58065629005432% dice_coef_multilabel of 75.24264454841614%\n",
            "Score for fold 2: loss of 0.07446722686290741; accuracy of 97.424978017807% iou_coef_multilabel of 66.99366569519043% dice_coef_multilabel of 73.48106503486633%\n",
            "Score for fold 3: loss of 0.07256180793046951; accuracy of 97.48387932777405% iou_coef_multilabel of 64.70680236816406% dice_coef_multilabel of 70.94796895980835%\n",
            "Score for fold 4: loss of 0.06639942526817322; accuracy of 97.35667705535889% iou_coef_multilabel of 60.659801959991455% dice_coef_multilabel of 67.65385866165161%\n",
            "Score for fold 5: loss of 0.06580083817243576; accuracy of 97.68750071525574% iou_coef_multilabel of 59.561049938201904% dice_coef_multilabel of 65.57979583740234%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((X_fold, Y_fold))\n",
        "  # Apply the data augmentation pipeline to both X_train and Y_train\n",
        "  augmented_train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x, y: x)\n",
        "  Y_train_augmented = augmented_train_data.map(lambda x, y: y)\n",
        "\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = np.asarray(list(Y_train_augmented.as_numpy_iterator()))\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxF74NqRTnYL"
      },
      "source": [
        "##Model data augmented kfold Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZiNs0HTnYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf9mFXu2TnYR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFb761bWTnYR"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "795ca727-865e-48a0-b64b-158404a4c6a6",
        "id": "GraQHW6qTnYR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.01390768215060234; accuracy of 99.5705783367157% iou_coef_multilabel of 86.49618029594421% dice_coef_multilabel of 89.0194833278656%\n",
            "Score for fold 2: loss of 0.03675416111946106; accuracy of 98.97904992103577% iou_coef_multilabel of 79.32537794113159% dice_coef_multilabel of 83.57661962509155%\n",
            "Score for fold 3: loss of 0.029099980369210243; accuracy of 99.24059510231018% iou_coef_multilabel of 80.83962798118591% dice_coef_multilabel of 84.87486839294434%\n",
            "Score for fold 4: loss of 0.023593006655573845; accuracy of 99.0794837474823% iou_coef_multilabel of 81.77911639213562% dice_coef_multilabel of 86.97380423545837%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-38a73e033f07>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mX_train_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_augmented\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mY_train_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mY_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_augmented\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_combined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train_combined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTzHLJ8PTnYR"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ac2eb2-682c-497c-8732-4766bb847d72",
        "id": "abm6yrrdTnYS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.013969536870718002; accuracy of 99.5184600353241% iou_coef_multilabel of 78.77731919288635% dice_coef_multilabel of 81.8413496017456%\n",
            "Score for fold 2: loss of 0.029770037159323692; accuracy of 99.06414747238159% iou_coef_multilabel of 80.31871914863586% dice_coef_multilabel of 84.64120626449585%\n",
            "Score for fold 3: loss of 0.026543816551566124; accuracy of 99.26268458366394% iou_coef_multilabel of 81.27678036689758% dice_coef_multilabel of 85.34112572669983%\n",
            "Score for fold 4: loss of 0.02282973751425743; accuracy of 99.13150072097778% iou_coef_multilabel of 81.2670886516571% dice_coef_multilabel of 86.49913668632507%\n",
            "Score for fold 5: loss of 0.02242366410791874; accuracy of 99.32538270950317% iou_coef_multilabel of 75.33347010612488% dice_coef_multilabel of 78.96603345870972%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHgVMw0RHdm_"
      }
    }
  ]
}