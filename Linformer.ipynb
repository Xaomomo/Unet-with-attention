{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM12jSGMERftREmvegChHju",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xaomomo/Unet-with-attention/blob/main/Linformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbiwq0rKHdag"
      },
      "source": [
        "#Linformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-BX8SLoI2Wu",
        "outputId": "675bc2b2-42d6-4747-dd1f-567c3b1c68d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\n",
            "To: /content/input.zip\n",
            "100% 367M/367M [00:08<00:00, 42.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex0pbxLdI2Ww"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ZAoQWII2Ww"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtw5FDriI2Wx"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu5trMBgI2Wx",
        "outputId": "bdb1f9ee-93ab-4da7-cd80-0e5d643c6a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3520, 128, 128)\n",
            "[0. 1. 2. 3.]\n",
            "(3520, 128, 128, 4)\n",
            "[0. 1.]\n"
          ]
        }
      ],
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "import PIL\n",
        "from PIL import Image\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/images/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    X[i,:,:,0]=im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST)\n",
        "mas=\"/Amasks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/Amasks/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    im=np.array(im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST))\n",
        "    im[im==85]=1\n",
        "    im[im==170]=2\n",
        "    im[im==255]=3\n",
        "    Y[i,:,:]=im\n",
        "import tensorflow as tf\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))\n",
        "Y=tf.keras.utils.to_categorical(Y)\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsDeC0BxI2Wy",
        "outputId": "0fa7d160-2087-4c79-9203-17919c189ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002\n",
            "003\n",
            "004\n",
            "005\n",
            "006\n",
            "007\n",
            "008\n",
            "009\n",
            "010\n",
            "10_85902_1\n",
            "10_85902_3\n",
            "14_85914_0\n",
            "27_86410_0\n",
            "29_86490_1\n",
            "29_86491_1\n",
            "36_86526_0\n",
            "40_86625_0\n",
            "4_85506_1\n",
            "7_85703_0\n"
          ]
        }
      ],
      "source": [
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "Siz=0\n",
        "id=0\n",
        "ri=0\n",
        "metadat=[]\n",
        "c=\"001\"\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x.find(\"radiopaedia\") == -1:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\")]\n",
        "  else:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\",9)]\n",
        "  if c!=x:\n",
        "    print(x)\n",
        "    metadat.append([id,Siz,ri])\n",
        "    ri=i\n",
        "    c=x\n",
        "    id+=1\n",
        "    Siz=0\n",
        "  Siz+=1\n",
        "metadat.append([id,Siz,ri])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqn8h312I2Wz"
      },
      "source": [
        "##New Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4YXvn3tI2Wz",
        "outputId": "cf0e7a19-dd81-48ac-a223-de58981cbe07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  1  2  4  5  7  8  9 11 12 13 14 15 17 18 19] [ 3  6 10 16]\n",
            "[ 0  1  3  5  6  7  8  9 10 11 12 13 15 16 18 19] [ 2  4 14 17]\n",
            "[ 2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19] [ 0  1  7 13]\n",
            "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17] [ 9 15 18 19]\n",
            "[ 0  1  2  3  4  6  7  9 10 13 14 15 16 17 18 19] [ 5  8 11 12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "IDX=[x for x in range(10)]\n",
        "IDY=[x for x in range(10)]\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  print(train_index, val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa-nwuWfI2W0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "IDX=[x for x in range(len(metadat))]\n",
        "train_index, val_index,_,_=train_test_split(\n",
        "   IDX,metadat, test_size=0.05, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8hWheeI2W0"
      },
      "outputs": [],
      "source": [
        "for i,x in enumerate(train_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_train=X[ri:ri+size]\n",
        "    Y_train=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_train=np.concatenate((X_train,X[ri:ri+size]),axis=0)\n",
        "    Y_train=np.concatenate((Y_train,Y[ri:ri+size]),axis=0)\n",
        "for i,x in enumerate(val_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_test=X[ri:ri+size]\n",
        "    Y_test=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_test=np.concatenate((X_test,X[ri:ri+size]),axis=0)\n",
        "    Y_test=np.concatenate((Y_test,Y[ri:ri+size]),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hiaYo2eJJHY"
      },
      "source": [
        "##Linformer init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHlvgwPMrBYF"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Keras-based multi-head attention layer.\"\"\"\n",
        "\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import constraints\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras.engine.base_layer import Layer\n",
        "from keras.layers import activation\n",
        "from keras.layers import core\n",
        "from keras.layers import regularization\n",
        "from keras.utils import tf_utils\n",
        "\n",
        "# isort: off\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "_CHR_IDX = string.ascii_lowercase\n",
        "\n",
        "def _build_attention_equation(rank, attn_axes):\n",
        "    \"\"\"Builds einsum equations for the attention computation.\n",
        "    Query, key, value inputs after projection are expected to have the shape as:\n",
        "    `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
        "    `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
        "    The attention operations can be generalized:\n",
        "    (1) Query-key dot product:\n",
        "    `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
        "    <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
        "    num_heads, <query attention dims>, <key attention dims>)`\n",
        "    (2) Combination:\n",
        "    `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
        "    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch\n",
        "    dims>, <query attention dims>, num_heads, channels)`\n",
        "    Args:\n",
        "      rank: Rank of query, key, value tensors.\n",
        "      attn_axes: List/tuple of axes, `[-1, rank)`,\n",
        "        that attention will be applied to.\n",
        "    Returns:\n",
        "      Einsum equations.\n",
        "    \"\"\"\n",
        "    target_notation = _CHR_IDX[:rank]\n",
        "    # `batch_dims` includes the head dim.\n",
        "    batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
        "    letter_offset = rank\n",
        "    source_notation = \"\"\n",
        "    for i in range(rank):\n",
        "        if i in batch_dims or i == rank - 1:\n",
        "            source_notation += target_notation[i]\n",
        "        else:\n",
        "            source_notation += _CHR_IDX[letter_offset]\n",
        "            letter_offset += 1\n",
        "\n",
        "    product_notation = \"\".join(\n",
        "        [target_notation[i] for i in batch_dims]\n",
        "        + [target_notation[i] for i in attn_axes]\n",
        "        + [source_notation[i] for i in attn_axes]\n",
        "    )\n",
        "    dot_product_equation = \"%s,%s->%s\" % (\n",
        "        source_notation,\n",
        "        target_notation,\n",
        "        product_notation,\n",
        "    )\n",
        "    attn_scores_rank = len(product_notation)\n",
        "    combine_equation = \"%s,%s->%s\" % (\n",
        "        product_notation,\n",
        "        source_notation,\n",
        "        target_notation,\n",
        "    )\n",
        "    return dot_product_equation, combine_equation, attn_scores_rank\n",
        "\n",
        "\n",
        "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
        "    \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
        "    input_str = \"\"\n",
        "    kernel_str = \"\"\n",
        "    output_str = \"\"\n",
        "    bias_axes = \"\"\n",
        "    letter_offset = 0\n",
        "    for i in range(free_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        input_str += char\n",
        "        output_str += char\n",
        "\n",
        "    letter_offset += free_dims\n",
        "    for i in range(bound_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        input_str += char\n",
        "        kernel_str += char\n",
        "\n",
        "    letter_offset += bound_dims\n",
        "    for i in range(output_dims):\n",
        "        char = _CHR_IDX[i + letter_offset]\n",
        "        kernel_str += char\n",
        "        output_str += char\n",
        "        bias_axes += char\n",
        "    equation = f\"{input_str},{kernel_str}->{output_str}\"\n",
        "    return equation, bias_axes, len(output_str)\n",
        "\n",
        "\n",
        "def _get_output_shape(output_rank, known_last_dims):\n",
        "    return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)\n",
        "\n",
        "class Linformer(Layer):\n",
        "    \"\"\"MultiHeadAttention layer.\n",
        "    This is an implementation of multi-headed attention as described in the\n",
        "    paper \"Attention is all you Need\" (Vaswani et al., 2017).\n",
        "    If `query`, `key,` `value` are the same, then\n",
        "    this is self-attention. Each timestep in `query` attends to the\n",
        "    corresponding sequence in `key`, and returns a fixed-width vector.\n",
        "    This layer first projects `query`, `key` and `value`. These are\n",
        "    (effectively) a list of tensors of length `num_attention_heads`, where the\n",
        "    corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n",
        "    `(batch_size, <key/value dimensions>, key_dim)`,\n",
        "    `(batch_size, <key/value dimensions>, value_dim)`.\n",
        "    Then, the query and key tensors are dot-producted and scaled. These are\n",
        "    softmaxed to obtain attention probabilities. The value tensors are then\n",
        "    interpolated by these probabilities, then concatenated back to a single\n",
        "    tensor.\n",
        "    Finally, the result tensor with the last dimension as value_dim can take an\n",
        "    linear projection and return.\n",
        "    When using `MultiHeadAttention` inside a custom layer, the custom layer must\n",
        "    implement its own `build()` method and call `MultiHeadAttention`'s\n",
        "    `_build_from_signature()` there.\n",
        "    This enables weights to be restored correctly when the model is loaded.\n",
        "    Examples:\n",
        "    Performs 1D cross-attention over two sequence inputs with an attention mask.\n",
        "    Returns the additional attention weights over heads.\n",
        "    >>> layer = MultiHeadAttention(num_heads=2, key_dim=2)\n",
        "    >>> target = tf.keras.Input(shape=[8, 16])\n",
        "    >>> source = tf.keras.Input(shape=[4, 16])\n",
        "    >>> output_tensor, weights = layer(target, source,\n",
        "    ...                                return_attention_scores=True)\n",
        "    >>> print(output_tensor.shape)\n",
        "    (None, 8, 16)\n",
        "    >>> print(weights.shape)\n",
        "    (None, 2, 8, 4)\n",
        "    Performs 2D self-attention over a 5D input tensor on axes 2 and 3.\n",
        "    >>> layer = MultiHeadAttention(\n",
        "    ...     num_heads=2, key_dim=2, attention_axes=(2, 3))\n",
        "    >>> input_tensor = tf.keras.Input(shape=[5, 3, 4, 16])\n",
        "    >>> output_tensor = layer(input_tensor, input_tensor)\n",
        "    >>> print(output_tensor.shape)\n",
        "    (None, 5, 3, 4, 16)\n",
        "    Args:\n",
        "      num_heads: Number of attention heads.\n",
        "      key_dim: Size of each attention head for query and key.\n",
        "      value_dim: Size of each attention head for value.\n",
        "      dropout: Dropout probability.\n",
        "      use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
        "      output_shape: The expected shape of an output tensor, besides the batch\n",
        "        and sequence dims. If not specified, projects back to the key feature\n",
        "        dim.\n",
        "      attention_axes: axes over which the attention is applied. `None` means\n",
        "        attention over all axes, but batch, heads, and features.\n",
        "      kernel_initializer: Initializer for dense layer kernels.\n",
        "      bias_initializer: Initializer for dense layer biases.\n",
        "      kernel_regularizer: Regularizer for dense layer kernels.\n",
        "      bias_regularizer: Regularizer for dense layer biases.\n",
        "      activity_regularizer: Regularizer for dense layer activity.\n",
        "      kernel_constraint: Constraint for dense layer kernels.\n",
        "      bias_constraint: Constraint for dense layer kernels.\n",
        "    Call arguments:\n",
        "      query: Query `Tensor` of shape `(B, T, dim)`.\n",
        "      value: Value `Tensor` of shape `(B, S, dim)`.\n",
        "      key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n",
        "        `value` for both `key` and `value`, which is the most common case.\n",
        "      attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "        attention to certain positions. The boolean mask specifies which query\n",
        "        elements can attend to which key elements, 1 indicates attention and 0\n",
        "        indicates no attention. Broadcasting can happen for the missing batch\n",
        "        dimensions and the head dimension.\n",
        "      return_attention_scores: A boolean to indicate whether the output should\n",
        "        be `(attention_output, attention_scores)` if `True`, or\n",
        "        `attention_output` if `False`. Defaults to `False`.\n",
        "      training: Python boolean indicating whether the layer should behave in\n",
        "        training mode (adding dropout) or in inference mode (no dropout).\n",
        "        Defaults to either using the training mode of the parent layer/model,\n",
        "        or False (inference) if there is no parent layer.\n",
        "      use_causal_mask: A boolean to indicate whether to apply a causal mask to\n",
        "        prevent tokens from attending to future tokens (e.g., used in a decoder\n",
        "        Transformer).\n",
        "    Returns:\n",
        "      attention_output: The result of the computation, of shape `(B, T, E)`,\n",
        "        where `T` is for target sequence shapes and `E` is the query input last\n",
        "        dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n",
        "        are projected to the shape specified by `output_shape`.\n",
        "      attention_scores: [Optional] multi-head attention coefficients over\n",
        "        attention axes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        key_dim,\n",
        "        value_dim=None,\n",
        "        dropout=0.0,\n",
        "        use_bias=True,\n",
        "        output_shape=None,\n",
        "        attention_axes=None,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        activity_regularizer=None,\n",
        "        kernel_constraint=None,\n",
        "        bias_constraint=None,\n",
        "        k_value=32,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self._num_heads = num_heads\n",
        "        self._key_dim = key_dim\n",
        "        self._value_dim = value_dim if value_dim else key_dim\n",
        "        self._dropout = dropout\n",
        "        self._use_bias = use_bias\n",
        "        self.k_value = k_value\n",
        "        self._output_shape = output_shape\n",
        "        self._kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self._bias_initializer = initializers.get(bias_initializer)\n",
        "        self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self._bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self._activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self._kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self._bias_constraint = constraints.get(bias_constraint)\n",
        "        if attention_axes is not None and not isinstance(\n",
        "            attention_axes, collections.abc.Sized\n",
        "        ):\n",
        "            self._attention_axes = (attention_axes,)\n",
        "        else:\n",
        "            self._attention_axes = attention_axes\n",
        "        self._built_from_signature = False\n",
        "        self._query_shape, self._key_shape, self._value_shape = None, None, None\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"num_heads\": self._num_heads,\n",
        "            \"key_dim\": self._key_dim,\n",
        "            \"value_dim\": self._value_dim,\n",
        "            \"dropout\": self._dropout,\n",
        "            \"use_bias\": self._use_bias,\n",
        "            \"output_shape\": self._output_shape,\n",
        "            \"attention_axes\": self._attention_axes,\n",
        "            \"kernel_initializer\": initializers.serialize(\n",
        "                self._kernel_initializer\n",
        "            ),\n",
        "            \"bias_initializer\": initializers.serialize(self._bias_initializer),\n",
        "            \"kernel_regularizer\": regularizers.serialize(\n",
        "                self._kernel_regularizer\n",
        "            ),\n",
        "            \"bias_regularizer\": regularizers.serialize(self._bias_regularizer),\n",
        "            \"activity_regularizer\": regularizers.serialize(\n",
        "                self._activity_regularizer\n",
        "            ),\n",
        "            \"kernel_constraint\": constraints.serialize(self._kernel_constraint),\n",
        "            \"bias_constraint\": constraints.serialize(self._bias_constraint),\n",
        "            \"query_shape\": self._query_shape,\n",
        "            \"key_shape\": self._key_shape,\n",
        "            \"value_shape\": self._value_shape,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # If the layer has a different build() function from the Keras default,\n",
        "        # we need to trigger the customized build to create weights.\n",
        "        query_shape = config.pop(\"query_shape\")\n",
        "        key_shape = config.pop(\"key_shape\")\n",
        "        value_shape = config.pop(\"value_shape\")\n",
        "        layer = cls(**config)\n",
        "        if None in [query_shape, key_shape, value_shape]:\n",
        "            logging.warning(\n",
        "                \"One of dimensions of the input shape is missing. It \"\n",
        "                \"should have been memorized when the layer was serialized. \"\n",
        "                \"%s is created without weights.\",\n",
        "                str(cls),\n",
        "            )\n",
        "        else:\n",
        "            layer._build_from_signature(query_shape, value_shape, key_shape)\n",
        "        return layer\n",
        "\n",
        "    def _build_from_signature(self, query, value, key=None):\n",
        "        \"\"\"Builds layers and variables.\n",
        "        Once the method is called, self._built_from_signature will be set to\n",
        "        True.\n",
        "        Args:\n",
        "          query: Query tensor or TensorShape.\n",
        "          value: Value tensor or TensorShape.\n",
        "          key: Key tensor or TensorShape.\n",
        "        \"\"\"\n",
        "        self._built_from_signature = True\n",
        "        if hasattr(query, \"shape\"):\n",
        "            self._query_shape = tf.TensorShape(query.shape)\n",
        "        else:\n",
        "            self._query_shape = tf.TensorShape(query)\n",
        "        if hasattr(value, \"shape\"):\n",
        "            self._value_shape = tf.TensorShape(value.shape)\n",
        "        else:\n",
        "            self._value_shape = tf.TensorShape(value)\n",
        "        if key is None:\n",
        "            self._key_shape = self._value_shape\n",
        "        elif hasattr(key, \"shape\"):\n",
        "            self._key_shape = tf.TensorShape(key.shape)\n",
        "        else:\n",
        "            self._key_shape = tf.TensorShape(key)\n",
        "\n",
        "        # Any setup work performed only once should happen in an `init_scope`\n",
        "        # to avoid creating symbolic Tensors that will later pollute any eager\n",
        "        # operations.\n",
        "        with tf_utils.maybe_init_scope(self):\n",
        "            free_dims = self._query_shape.rank - 1\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                free_dims, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._query_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._key_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"query\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                self._key_shape.rank - 1, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._key_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._key_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"key\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "            self._proj=tf.keras.layers.EinsumDense(\"bjhd,kj->bkhd\",output_shape=(self.k_value,None,None))\n",
        "\n",
        "            einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "                self._value_shape.rank - 1, bound_dims=1, output_dims=2\n",
        "            )\n",
        "            self._value_dense = core.EinsumDense(\n",
        "                einsum_equation,\n",
        "                output_shape=_get_output_shape(\n",
        "                    output_rank - 1, [self._num_heads, self._value_dim]\n",
        "                ),\n",
        "                bias_axes=bias_axes if self._use_bias else None,\n",
        "                name=\"value\",\n",
        "                **self._get_common_kwargs_for_sublayer(),\n",
        "            )\n",
        "\n",
        "            # Builds the attention computations for multi-head dot product\n",
        "            # attention.  These computations could be wrapped into the keras\n",
        "            # attention layer once it supports mult-head einsum computations.\n",
        "            self._build_attention(output_rank)\n",
        "            self._output_dense = self._make_output_dense(\n",
        "                free_dims,\n",
        "                self._get_common_kwargs_for_sublayer(),\n",
        "                \"attention_output\",\n",
        "            )\n",
        "\n",
        "    def _get_common_kwargs_for_sublayer(self):\n",
        "        common_kwargs = dict(\n",
        "            kernel_regularizer=self._kernel_regularizer,\n",
        "            bias_regularizer=self._bias_regularizer,\n",
        "            activity_regularizer=self._activity_regularizer,\n",
        "            kernel_constraint=self._kernel_constraint,\n",
        "            bias_constraint=self._bias_constraint,\n",
        "        )\n",
        "        # Create new clone of kernel/bias initializer, so that we don't reuse\n",
        "        # the initializer instance, which could lead to same init value since\n",
        "        # initializer is stateless.\n",
        "        kernel_initializer = self._kernel_initializer.__class__.from_config(\n",
        "            self._kernel_initializer.get_config()\n",
        "        )\n",
        "        bias_initializer = self._bias_initializer.__class__.from_config(\n",
        "            self._bias_initializer.get_config()\n",
        "        )\n",
        "        common_kwargs[\"kernel_initializer\"] = kernel_initializer\n",
        "        common_kwargs[\"bias_initializer\"] = bias_initializer\n",
        "        return common_kwargs\n",
        "\n",
        "    def _make_output_dense(self, free_dims, common_kwargs, name=None):\n",
        "        \"\"\"Builds the output projection matrix.\n",
        "        Args:\n",
        "          free_dims: Number of free dimensions for einsum equation building.\n",
        "          common_kwargs: Common keyword arguments for einsum layer.\n",
        "          name: Name for the projection layer.\n",
        "        Returns:\n",
        "          Projection layer.\n",
        "        \"\"\"\n",
        "        if self._output_shape:\n",
        "            if not isinstance(self._output_shape, collections.abc.Sized):\n",
        "                output_shape = [self._output_shape]\n",
        "            else:\n",
        "                output_shape = self._output_shape\n",
        "        else:\n",
        "            output_shape = [self._query_shape[-1]]\n",
        "        einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "            free_dims, bound_dims=2, output_dims=len(output_shape)\n",
        "        )\n",
        "        return core.EinsumDense(\n",
        "            einsum_equation,\n",
        "            output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
        "            bias_axes=bias_axes if self._use_bias else None,\n",
        "            name=name,\n",
        "            **common_kwargs,\n",
        "        )\n",
        "\n",
        "    def _build_attention(self, rank):\n",
        "        \"\"\"Builds multi-head dot-product attention computations.\n",
        "        This function builds attributes necessary for `_compute_attention` to\n",
        "        customize attention computation to replace the default dot-product\n",
        "        attention.\n",
        "        Args:\n",
        "          rank: the rank of query, key, value tensors.\n",
        "        \"\"\"\n",
        "        if self._attention_axes is None:\n",
        "            self._attention_axes = tuple(range(1, rank - 2))\n",
        "        else:\n",
        "            self._attention_axes = tuple(self._attention_axes)\n",
        "        (\n",
        "            self._dot_product_equation,\n",
        "            self._combine_equation,\n",
        "            attn_scores_rank,\n",
        "        ) = _build_attention_equation(rank, attn_axes=self._attention_axes)\n",
        "        norm_axes = tuple(\n",
        "            range(\n",
        "                attn_scores_rank - len(self._attention_axes), attn_scores_rank\n",
        "            )\n",
        "        )\n",
        "        self._softmax = activation.Softmax(axis=norm_axes)\n",
        "        self._dropout_layer = regularization.Dropout(rate=self._dropout)\n",
        "\n",
        "    def _masked_softmax(self, attention_scores, attention_mask=None):\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        # `attention_scores` = [B, N, T, S]\n",
        "        if attention_mask is not None:\n",
        "            # The expand dim happens starting from the `num_heads` dimension,\n",
        "            # (<batch_dims>, num_heads, <query_attention_dims,\n",
        "            # key_attention_dims>)\n",
        "            mask_expansion_axis = -len(self._attention_axes) * 2 - 1\n",
        "            for _ in range(\n",
        "                len(attention_scores.shape) - len(attention_mask.shape)\n",
        "            ):\n",
        "                attention_mask = tf.expand_dims(\n",
        "                    attention_mask, axis=mask_expansion_axis\n",
        "                )\n",
        "        return self._softmax(attention_scores, attention_mask)\n",
        "\n",
        "    def _compute_attention(\n",
        "        self, query, key, value, attention_mask=None, training=None\n",
        "    ):\n",
        "        \"\"\"Applies Dot-product attention with query, key, value tensors.\n",
        "        This function defines the computation inside `call` with projected\n",
        "        multi-head Q, K, V inputs. Users can override this function for\n",
        "        customized attention implementation.\n",
        "        Args:\n",
        "          query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          key: Projected key `Tensor` of shape `(B, S, N, key_dim)`.\n",
        "          value: Projected value `Tensor` of shape `(B, S, N, value_dim)`.\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions. It is generally not needed if the\n",
        "            `query` and `value` (and/or `key`) are masked.\n",
        "          training: Python boolean indicating whether the layer should behave in\n",
        "            training mode (adding dropout) or in inference mode (doing nothing).\n",
        "        Returns:\n",
        "          attention_output: Multi-headed outputs of attention computation.\n",
        "          attention_scores: Multi-headed attention weights.\n",
        "        \"\"\"\n",
        "        # Note: Applying scalar multiply at the smaller end of einsum improves\n",
        "        # XLA performance, but may introduce slight numeric differences in\n",
        "        # the Transformer attention head.\n",
        "        query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "        # attention scores.\n",
        "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
        "        attention_scores = self._masked_softmax(\n",
        "            attention_scores, attention_mask\n",
        "        )\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_scores_dropout = self._dropout_layer(\n",
        "            attention_scores, training=training\n",
        "        )\n",
        "\n",
        "        # `context_layer` = [B, T, N, H]\n",
        "        attention_output = tf.einsum(\n",
        "            self._combine_equation, attention_scores_dropout, value\n",
        "        )\n",
        "        #import sys\n",
        "        #sys.exit()\n",
        "        return attention_output, attention_scores\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        query,\n",
        "        value,\n",
        "        key=None,\n",
        "        attention_mask=None,\n",
        "        return_attention_scores=False,\n",
        "        training=None,\n",
        "        use_causal_mask=False,\n",
        "    ):\n",
        "        attention_mask = self._compute_attention_mask(\n",
        "            query,\n",
        "            value,\n",
        "            key=key,\n",
        "            attention_mask=attention_mask,\n",
        "            use_causal_mask=use_causal_mask,\n",
        "        )\n",
        "\n",
        "        if not self._built_from_signature:\n",
        "            self._build_from_signature(query=query, value=value, key=key)\n",
        "        if key is None:\n",
        "            key = value\n",
        "\n",
        "        query_is_ragged = isinstance(query, tf.RaggedTensor)\n",
        "        if query_is_ragged:\n",
        "            query_lengths = query.nested_row_lengths()\n",
        "            query = query.to_tensor()\n",
        "\n",
        "        key_is_ragged = isinstance(key, tf.RaggedTensor)\n",
        "        value_is_ragged = isinstance(value, tf.RaggedTensor)\n",
        "        if key_is_ragged and value_is_ragged:\n",
        "            # Ensure they have the same shape.\n",
        "            bounding_shape = tf.math.maximum(\n",
        "                key.bounding_shape(), value.bounding_shape()\n",
        "            )\n",
        "            key = key.to_tensor(shape=bounding_shape)\n",
        "            value = value.to_tensor(shape=bounding_shape)\n",
        "        elif key_is_ragged:\n",
        "            key = key.to_tensor(shape=tf.shape(value))\n",
        "        elif value_is_ragged:\n",
        "            value = value.to_tensor(shape=tf.shape(key))\n",
        "\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        # `query` = [B, T, N ,H]\n",
        "        query = self._query_dense(query)\n",
        "        # `key` = [B, S, N, H]\n",
        "        key = self._key_dense(key)\n",
        "        key = self._proj(key)\n",
        "        # `value` = [B, S, N, H]\n",
        "        value = self._value_dense(value)\n",
        "        value = self._proj(value)\n",
        "        attention_output, attention_scores = self._compute_attention(\n",
        "            query, key, value, attention_mask, training\n",
        "        )\n",
        "        attention_output = self._output_dense(attention_output)\n",
        "\n",
        "        if query_is_ragged:\n",
        "            attention_output = tf.RaggedTensor.from_tensor(\n",
        "                attention_output, lengths=query_lengths\n",
        "            )\n",
        "\n",
        "        if return_attention_scores:\n",
        "            return attention_output, attention_scores\n",
        "        return attention_output\n",
        "\n",
        "    def _compute_attention_mask(\n",
        "        self, query, value, key=None, attention_mask=None, use_causal_mask=False\n",
        "    ):\n",
        "        \"\"\"Computes the attention mask, using the Keras masks of the inputs.\n",
        "        * The `query`'s mask is reshaped from [B, T] to [B, T, 1].\n",
        "        * The `value`'s mask is reshaped from [B, S] to [B, 1, S].\n",
        "        * The `key`'s mask is reshaped from [B, S] to [B, 1, S]. The `key`'s\n",
        "          mask is ignored if `key` is `None` or if `key is value`.\n",
        "        * If `use_causal_mask=True`, then the causal mask is computed. Its shape\n",
        "          is [1, T, S].\n",
        "        All defined masks are merged using a logical AND operation (`&`).\n",
        "        In general, if the `query` and `value` are masked, then there is no need\n",
        "        to define the `attention_mask`.\n",
        "        Args:\n",
        "          query: Projected query `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          key: Projected key `Tensor` of shape `(B, T, N, key_dim)`.\n",
        "          value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions.\n",
        "          use_causal_mask: A boolean to indicate whether to apply a causal mask\n",
        "            to prevent tokens from attending to future tokens (e.g., used in a\n",
        "            decoder Transformer).\n",
        "        Returns:\n",
        "          attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
        "            attention to certain positions, based on the Keras masks of the\n",
        "            `query`, `key`, `value`, and `attention_mask` tensors, and the\n",
        "            causal mask if `use_causal_mask=True`.\n",
        "        \"\"\"\n",
        "        query_mask = getattr(query, \"_keras_mask\", None)\n",
        "        value_mask = getattr(value, \"_keras_mask\", None)\n",
        "        key_mask = getattr(key, \"_keras_mask\", None)\n",
        "        auto_mask = None\n",
        "        if query_mask is not None:\n",
        "            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, T = max query length\n",
        "            auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, T, 1]\n",
        "        if value_mask is not None:\n",
        "            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n",
        "            # B = batch size, S == max value length\n",
        "            mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if key_mask is not None:\n",
        "            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n",
        "            # B == batch size, S == max key length == max value length\n",
        "            mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if use_causal_mask:\n",
        "            # the shape of the causal mask is [1, T, S]\n",
        "            mask = self._compute_causal_mask(query, value)\n",
        "            auto_mask = mask if auto_mask is None else auto_mask & mask\n",
        "        if auto_mask is not None:\n",
        "            # merge attention_mask & automatic mask, to shape [B, T, S]\n",
        "            attention_mask = (\n",
        "                auto_mask\n",
        "                if attention_mask is None\n",
        "                else tf.cast(attention_mask, bool) & auto_mask\n",
        "            )\n",
        "        return attention_mask\n",
        "\n",
        "    def _compute_causal_mask(self, query, value=None):\n",
        "        \"\"\"Computes a causal mask (e.g., for masked self-attention layers).\n",
        "        For example, if query and value both contain sequences of length 4,\n",
        "        this function returns a boolean `Tensor` equal to:\n",
        "        ```\n",
        "        [[[True,  False, False, False],\n",
        "          [True,  True,  False, False],\n",
        "          [True,  True,  True,  False],\n",
        "          [True,  True,  True,  True]]]\n",
        "        ```\n",
        "        Args:\n",
        "          query: query `Tensor` of shape `(B, T, ...)`.\n",
        "          value: value `Tensor` of shape `(B, S, ...)` (optional, defaults to\n",
        "          query).\n",
        "        Returns:\n",
        "          mask: a boolean `Tensor` of shape [1, T, S] containing a lower\n",
        "                triangular matrix of shape [T, S].\n",
        "        \"\"\"\n",
        "        q_seq_length = tf.shape(query)[1]\n",
        "        v_seq_length = q_seq_length if value is None else tf.shape(value)[1]\n",
        "        return tf.linalg.band_part(  # creates a lower triangular matrix\n",
        "            tf.ones((1, q_seq_length, v_seq_length), tf.bool), -1, 0\n",
        "        )\n",
        "\n",
        "    def compute_output_shape(self, query_shape, value_shape, key_shape=None):\n",
        "\n",
        "        if key_shape is None:\n",
        "            key_shape = value_shape\n",
        "\n",
        "        query_shape = tf.TensorShape(query_shape)\n",
        "        value_shape = tf.TensorShape(value_shape)\n",
        "        key_shape = tf.TensorShape(key_shape)\n",
        "\n",
        "        if query_shape[-1] != value_shape[-1]:\n",
        "            raise ValueError(\n",
        "                \"The last dimension of `query_shape` and `value_shape` \"\n",
        "                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n",
        "                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n",
        "            )\n",
        "\n",
        "        if value_shape[1:-1] != key_shape[1:-1]:\n",
        "            raise ValueError(\n",
        "                \"All dimensions of `value` and `key`, except the last one, \"\n",
        "                f\"must be equal. Received {value_shape} and \"\n",
        "                f\"{key_shape}\"\n",
        "            )\n",
        "\n",
        "        if self._output_shape:\n",
        "            return query_shape[:-1].concatenate(self._output_shape)\n",
        "\n",
        "        return query_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuU1I__9PLzd"
      },
      "source": [
        "##Linformer as keras multihead subclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhMT80KNPLCp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers.attention.multi_head_attention import MultiHeadAttention\n",
        "from keras.utils import tf_utils\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Linformer(MultiHeadAttention):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    self.k_value = kwargs.pop('k_value', 128)\n",
        "    super().__init__(*args, **kwargs)\n",
        "  def _build_from_signature(self, query, value, key=None):\n",
        "    with tf_utils.maybe_init_scope(self):\n",
        "      self._proj=tf.keras.layers.EinsumDense(\"bjhd,kj->bkhd\",output_shape=(self.k_value,None,None))\n",
        "    super()._build_from_signature(query, value, key=None)\n",
        "\n",
        "  def call(\n",
        "        self,\n",
        "        query,\n",
        "        value,\n",
        "        key=None,\n",
        "        attention_mask=None,\n",
        "        return_attention_scores=False,\n",
        "        training=None,\n",
        "        use_causal_mask=False,\n",
        "        ):\n",
        "        attention_mask = self._compute_attention_mask(\n",
        "            query,\n",
        "            value,\n",
        "            key=key,\n",
        "            attention_mask=attention_mask,\n",
        "            use_causal_mask=use_causal_mask,\n",
        "        )\n",
        "\n",
        "        if not self._built_from_signature:\n",
        "            self._build_from_signature(query=query, value=value, key=key)\n",
        "        if key is None:\n",
        "            key = value\n",
        "\n",
        "        query_is_ragged = isinstance(query, tf.RaggedTensor)\n",
        "        if query_is_ragged:\n",
        "            query_lengths = query.nested_row_lengths()\n",
        "            query = query.to_tensor()\n",
        "\n",
        "        key_is_ragged = isinstance(key, tf.RaggedTensor)\n",
        "        value_is_ragged = isinstance(value, tf.RaggedTensor)\n",
        "        if key_is_ragged and value_is_ragged:\n",
        "            # Ensure they have the same shape.\n",
        "            bounding_shape = tf.math.maximum(\n",
        "                key.bounding_shape(), value.bounding_shape()\n",
        "            )\n",
        "            key = key.to_tensor(shape=bounding_shape)\n",
        "            value = value.to_tensor(shape=bounding_shape)\n",
        "        elif key_is_ragged:\n",
        "            key = key.to_tensor(shape=tf.shape(value))\n",
        "        elif value_is_ragged:\n",
        "            value = value.to_tensor(shape=tf.shape(key))\n",
        "\n",
        "        #   N = `num_attention_heads`\n",
        "        #   H = `size_per_head`\n",
        "        # `query` = [B, T, N ,H]\n",
        "        query = self._query_dense(query)\n",
        "        # `key` = [B, S, N, H]\n",
        "        key = self._key_dense(key)\n",
        "        key = self._proj(key)\n",
        "        # `value` = [B, S, N, H]\n",
        "        value = self._value_dense(value)\n",
        "        value = self._proj(value)\n",
        "        attention_output, attention_scores = self._compute_attention(\n",
        "            query, key, value, attention_mask, training\n",
        "        )\n",
        "        attention_output = self._output_dense(attention_output)\n",
        "\n",
        "        if query_is_ragged:\n",
        "            attention_output = tf.RaggedTensor.from_tensor(\n",
        "                attention_output, lengths=query_lengths\n",
        "        )\n",
        "\n",
        "        if return_attention_scores:\n",
        "            return attention_output, attention_scores\n",
        "        return attention_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKms-Jo8H-kn"
      },
      "source": [
        "##Patch wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUySwAEoH-ko"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W65NvnvZH-kp"
      },
      "outputs": [],
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6\n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kli_5DCBH-kp"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def iou_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
        "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "  return iou\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
        "  return dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmLKTdyUH-kq"
      },
      "outputs": [],
      "source": [
        "def dice_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      #None,128,128,1\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      #None,128,128\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += dice_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oopp7EhGH-kq"
      },
      "outputs": [],
      "source": [
        "def iou_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += iou_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cih6HuiDH-kr"
      },
      "outputs": [],
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (3, 3), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (1, 1), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx_Lky4g-HLp"
      },
      "outputs": [],
      "source": [
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a6nHZGjH-kr"
      },
      "outputs": [],
      "source": [
        "def vit(X,n_heads=16,psize=16,k=256):\n",
        "  npatch=int(X.shape[1]/psize)\n",
        "  for i in range(npatch):\n",
        "    st=i*psize\n",
        "    end=st+psize\n",
        "    for j in range(npatch):\n",
        "      st2=j*psize\n",
        "      end2=st2+psize\n",
        "      patch= X[:,st:end,st2:end2,:]\n",
        "      s1=patch.shape[1]\n",
        "      s2=patch.shape[2]\n",
        "      s3=patch.shape[3]\n",
        "      flat=s1*s2\n",
        "      kdim=int(s3/n_heads)\n",
        "      patch= tf.reshape(patch,[-1,flat,s3])\n",
        "      pm=Linformer(num_heads=n_heads,key_dim=kdim,attention_axes=(1),dropout=0.2,k_value=k)(patch,patch)\n",
        "      pm=tf.reshape(pm,[-1,s1,s2,s3])\n",
        "      if j==0:\n",
        "        A=pm\n",
        "      else:\n",
        "        A=tf.concat([A,pm],2)\n",
        "    if i==0:\n",
        "      V=A\n",
        "    else:\n",
        "      V=tf.concat([V,A],1)\n",
        "  return V\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUkd8XOvH-ks"
      },
      "source": [
        "Vit 2, with dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Kec4yNEH-ks"
      },
      "outputs": [],
      "source": [
        "def vit(X,n_heads=16,psize=16):\n",
        "  npatch=int(X.shape[1]/psize)\n",
        "  temp=0.5\n",
        "  for i in range(npatch):\n",
        "    st=i*psize\n",
        "    end=st+psize\n",
        "    for j in range(npatch):\n",
        "      st2=j*psize\n",
        "      end2=st2+psize\n",
        "      patch= X[:,st:end,st2:end2,:]\n",
        "      random.uniform(0,1)\n",
        "      if random.uniform(0,1)> temp:\n",
        "        pm=tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=3, attention_axes=(1, 2))(patch,patch)\n",
        "      else:\n",
        "        pm=patch\n",
        "      if j==0:\n",
        "        A=pm\n",
        "      else:\n",
        "        A=tf.concat([A,pm],2)\n",
        "    if i==0:\n",
        "      V=A\n",
        "    else:\n",
        "      V=tf.concat([V,A],1)\n",
        "  return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXKkrPRH-kx"
      },
      "source": [
        "### Vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h9vieygH-kx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  nheads=4\n",
        "  sizep=32\n",
        "  gr=4\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=gr)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=gr)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=gr)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5,z5 = conv_block(p4,[256,256],0.3,group=gr)\n",
        "\n",
        "  #Expansive path\n",
        "  s1=z4.shape[1]\n",
        "  s2=z4.shape[2]\n",
        "  s3=z4.shape[3]\n",
        "  flat=s1*s2\n",
        "  z4= tf.reshape(z4,[-1,flat,s3])\n",
        "  m1= Linformer(num_heads=nheads, key_dim=int(s3/nheads), attention_axes=(1))(z4,z4)\n",
        "  m1=tf.reshape(m1,[-1,s1,s2,s3])\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2,group=gr)\n",
        "\n",
        "  m2= vit(z3,nheads,sizep)\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2,group=gr)\n",
        "\n",
        "  m3= vit(z2,nheads,sizep)\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32],group=gr)\n",
        "\n",
        "  m4= vit(z1,nheads,sizep)\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=gr)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_coef_multilabel,dice_coef_multilabel])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUN1FbBxpx23"
      },
      "source": [
        "###Linformer 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi15V9Ihp8SU"
      },
      "outputs": [],
      "source": [
        "def Linformer_lay(M,nheads,k=128):\n",
        "  s1=M.shape[1]\n",
        "  s2=M.shape[2]\n",
        "  s3=M.shape[3]\n",
        "  flat=s1*s2\n",
        "  M= tf.reshape(M,[-1,flat,s3])\n",
        "  m= Linformer(num_heads=nheads, key_dim=int(s3/nheads), attention_axes=(1),k_value=k)(M,M)\n",
        "  m=tf.reshape(m,[-1,s1,s2,s3])\n",
        "  return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-3KKmhCp4Ue"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  nheads=4\n",
        "  sizep=32\n",
        "  gr=4\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=gr)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=gr)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=gr)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5,z5 = conv_block(p4,[256,256],0.3,group=gr)\n",
        "\n",
        "  #Expansive path\n",
        "\n",
        "  m1= Linformer_lay(z4,nheads)\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2,group=gr)\n",
        "\n",
        "  m2= Linformer_lay(z3,nheads)\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2,group=gr)\n",
        "\n",
        "  m3= Linformer_lay(z2,nheads)\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32],group=gr)\n",
        "\n",
        "  m4= Linformer_lay(z1,nheads)\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=gr)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_coef_multilabel,dice_coef_multilabel])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlE3bWkIpvDE"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UGu9XXsJmbg",
        "outputId": "0877389a-7654-4ed3-cc68-6a5b864cece6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "214/214 [==============================] - 132s 344ms/step - loss: 0.5585 - accuracy: 0.8376 - iou_coef_multilabel: 0.2579 - dice_coef_multilabel: 0.3317\n",
            "Epoch 2/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0742 - accuracy: 0.9855 - iou_coef_multilabel: 0.4980 - dice_coef_multilabel: 0.5614\n",
            "Epoch 3/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0364 - accuracy: 0.9916 - iou_coef_multilabel: 0.5818 - dice_coef_multilabel: 0.6356\n",
            "Epoch 4/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0260 - accuracy: 0.9931 - iou_coef_multilabel: 0.6200 - dice_coef_multilabel: 0.6686\n",
            "Epoch 5/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0208 - accuracy: 0.9940 - iou_coef_multilabel: 0.6454 - dice_coef_multilabel: 0.6904\n",
            "Epoch 6/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0186 - accuracy: 0.9943 - iou_coef_multilabel: 0.6613 - dice_coef_multilabel: 0.7040\n",
            "Epoch 7/25\n",
            "214/214 [==============================] - 69s 320ms/step - loss: 0.0166 - accuracy: 0.9947 - iou_coef_multilabel: 0.6764 - dice_coef_multilabel: 0.7170\n",
            "Epoch 8/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0153 - accuracy: 0.9949 - iou_coef_multilabel: 0.6872 - dice_coef_multilabel: 0.7266\n",
            "Epoch 9/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0150 - accuracy: 0.9949 - iou_coef_multilabel: 0.6941 - dice_coef_multilabel: 0.7330\n",
            "Epoch 10/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0138 - accuracy: 0.9952 - iou_coef_multilabel: 0.7058 - dice_coef_multilabel: 0.7434\n",
            "Epoch 11/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0133 - accuracy: 0.9953 - iou_coef_multilabel: 0.7135 - dice_coef_multilabel: 0.7506\n",
            "Epoch 12/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0130 - accuracy: 0.9954 - iou_coef_multilabel: 0.7206 - dice_coef_multilabel: 0.7570\n",
            "Epoch 13/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0125 - accuracy: 0.9955 - iou_coef_multilabel: 0.7291 - dice_coef_multilabel: 0.7650\n",
            "Epoch 14/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0121 - accuracy: 0.9956 - iou_coef_multilabel: 0.7357 - dice_coef_multilabel: 0.7712\n",
            "Epoch 15/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0120 - accuracy: 0.9956 - iou_coef_multilabel: 0.7415 - dice_coef_multilabel: 0.7767\n",
            "Epoch 16/25\n",
            "214/214 [==============================] - 69s 320ms/step - loss: 0.0115 - accuracy: 0.9958 - iou_coef_multilabel: 0.7513 - dice_coef_multilabel: 0.7859\n",
            "Epoch 17/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0113 - accuracy: 0.9958 - iou_coef_multilabel: 0.7556 - dice_coef_multilabel: 0.7899\n",
            "Epoch 18/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0111 - accuracy: 0.9959 - iou_coef_multilabel: 0.7630 - dice_coef_multilabel: 0.7972\n",
            "Epoch 19/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0109 - accuracy: 0.9959 - iou_coef_multilabel: 0.7677 - dice_coef_multilabel: 0.8013\n",
            "Epoch 20/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0107 - accuracy: 0.9960 - iou_coef_multilabel: 0.7757 - dice_coef_multilabel: 0.8093\n",
            "Epoch 21/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0104 - accuracy: 0.9960 - iou_coef_multilabel: 0.7826 - dice_coef_multilabel: 0.8158\n",
            "Epoch 22/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0104 - accuracy: 0.9960 - iou_coef_multilabel: 0.7858 - dice_coef_multilabel: 0.8188\n",
            "Epoch 23/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0102 - accuracy: 0.9961 - iou_coef_multilabel: 0.7933 - dice_coef_multilabel: 0.8259\n",
            "Epoch 24/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0099 - accuracy: 0.9962 - iou_coef_multilabel: 0.7987 - dice_coef_multilabel: 0.8310\n",
            "Epoch 25/25\n",
            "214/214 [==============================] - 69s 321ms/step - loss: 0.0098 - accuracy: 0.9962 - iou_coef_multilabel: 0.8014 - dice_coef_multilabel: 0.8335\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc2e80dfc70>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=create_model()\n",
        "model.fit(X_train,Y_train,batch_size=16,epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yNkgZfLmnD8",
        "outputId": "ccd13f47-5786-4450-f479-454e75c8e585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 0: loss of 0.11757280677556992; accuracy of 96.79576754570007% iou_coef_multilabel of 71.6090202331543% dice_coef_multilabel of 79.81376647949219%\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ_noHHMsOEk"
      },
      "source": [
        "Patch size=32 k=32:\n",
        "\n",
        "Score for fold 0: loss of 0.02660856954753399; accuracy of 99.21053647994995% iou_coef_multilabel of 73.97453188896179% dice_coef_multilabel of 77.59955525398254%\n",
        "\n",
        "No Patch k=128:\n",
        "\n",
        "Score for fold 0: loss of 0.03675605729222298; accuracy of 98.8860547542572% iou_coef_multilabel of 69.78949904441833% dice_coef_multilabel of 74.2724359035492%\n",
        "\n",
        "Patch size=32 k=256:\n",
        "Score for fold 0: loss of 0.11757280677556992; accuracy of 96.79576754570007% iou_coef_multilabel of 71.6090202331543% dice_coef_multilabel of 79.81376647949219%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2D-muOYe_Pj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trswxiGSe_Pl"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWRqddndzFCR"
      },
      "source": [
        "####Patch test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T-tvQ42fpKS"
      },
      "source": [
        "k=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qUsIi9Se4PD",
        "outputId": "1dee6890-0a96-4028-8883-3a8f6e26bd6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.019922519102692604; accuracy of 99.3967354297638% iou_coef_multilabel of 77.20364928245544% dice_coef_multilabel of 80.38443922996521%\n",
            "Score for fold 2: loss of 0.032541245222091675; accuracy of 99.03221726417542% iou_coef_multilabel of 77.34413743019104% dice_coef_multilabel of 81.7846953868866%\n",
            "Score for fold 3: loss of 0.032382525503635406; accuracy of 99.13042187690735% iou_coef_multilabel of 76.8735408782959% dice_coef_multilabel of 81.07832670211792%\n",
            "Score for fold 4: loss of 0.0299893356859684; accuracy of 98.92753958702087% iou_coef_multilabel of 75.21778345108032% dice_coef_multilabel of 80.6958019733429%\n",
            "Score for fold 5: loss of 0.022455349564552307; accuracy of 99.26480650901794% iou_coef_multilabel of 74.87989068031311% dice_coef_multilabel of 78.63270044326782%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u-iwkU7t4ak",
        "outputId": "fef1ccc3-925e-4ab2-e38d-c43f2c6879df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.01698482222855091; accuracy of 99.46913719177246% iou_coef_multilabel of 80.59046268463135% dice_coef_multilabel of 83.47237706184387%\n",
            "Score for fold 2: loss of 0.03444297984242439; accuracy of 99.04292821884155% iou_coef_multilabel of 80.84890246391296% dice_coef_multilabel of 85.20692586898804%\n",
            "Score for fold 3: loss of 0.03221513703465462; accuracy of 99.17513728141785% iou_coef_multilabel of 80.00396490097046% dice_coef_multilabel of 84.08617377281189%\n",
            "Score for fold 4: loss of 0.030498819425702095; accuracy of 99.04159307479858% iou_coef_multilabel of 80.91708421707153% dice_coef_multilabel of 85.87648272514343%\n",
            "Score for fold 5: loss of 0.028706222772598267; accuracy of 99.25541877746582% iou_coef_multilabel of 79.54670190811157% dice_coef_multilabel of 82.86161422729492%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp5y4WBGfweq"
      },
      "source": [
        "k=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kmNTemGfvkE"
      },
      "outputs": [],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "i2HqP5AsfvkH",
        "outputId": "223edcd0-e993-4a38-b8cb-83c904d63c31"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9ce86cec98bc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_fold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbVD7hu9zIad"
      },
      "source": [
        "####No Patch test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOYSXDX_zOMg",
        "outputId": "dbe7fb28-8dfc-4f11-a560-9c36f15753d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.014879913069307804; accuracy of 99.50977563858032% iou_coef_multilabel of 75.01700520515442% dice_coef_multilabel of 78.04171442985535%\n",
            "Score for fold 2: loss of 0.03799952194094658; accuracy of 98.88894557952881% iou_coef_multilabel of 79.5952320098877% dice_coef_multilabel of 83.83917808532715%\n",
            "Score for fold 3: loss of 0.027269238606095314; accuracy of 99.2344617843628% iou_coef_multilabel of 78.04872393608093% dice_coef_multilabel of 82.01154470443726%\n",
            "Score for fold 4: loss of 0.035040128976106644; accuracy of 98.67711067199707% iou_coef_multilabel of 70.38484811782837% dice_coef_multilabel of 76.41092538833618%\n",
            "Score for fold 5: loss of 0.04386671259999275; accuracy of 99.13702011108398% iou_coef_multilabel of 76.85499787330627% dice_coef_multilabel of 80.35175204277039%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTKk58bXzOMi",
        "outputId": "4ad5e4af-d9db-4da7-9b84-2045fb9b26cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.018669821321964264; accuracy of 99.44900274276733% iou_coef_multilabel of 82.00370669364929% dice_coef_multilabel of 84.69013571739197%\n",
            "Score for fold 2: loss of 0.03746144101023674; accuracy of 98.98282289505005% iou_coef_multilabel of 79.05458807945251% dice_coef_multilabel of 83.39856266975403%\n",
            "Score for fold 3: loss of 0.029935374855995178; accuracy of 99.2314100265503% iou_coef_multilabel of 81.18868470191956% dice_coef_multilabel of 85.30572056770325%\n",
            "Score for fold 4: loss of 0.028801847249269485; accuracy of 99.00671243667603% iou_coef_multilabel of 81.22592568397522% dice_coef_multilabel of 86.3890290260315%\n",
            "Score for fold 5: loss of 0.029414121061563492; accuracy of 99.23427700996399% iou_coef_multilabel of 77.37159729003906% dice_coef_multilabel of 80.87003231048584%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2twGmSlDwB",
        "outputId": "cde57b75-624e-4dbc-fb74-9cbeec70658d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.01531380508095026; accuracy of 99.50594902038574% iou_coef_multilabel of 80.83136677742004% dice_coef_multilabel of 83.69719982147217%\n",
            "Score for fold 2: loss of 0.04106637462973595; accuracy of 98.89675974845886% iou_coef_multilabel of 81.13894462585449% dice_coef_multilabel of 85.50359010696411%\n",
            "Score for fold 3: loss of 0.031158242374658585; accuracy of 99.2102563381195% iou_coef_multilabel of 80.927973985672% dice_coef_multilabel of 85.0109875202179%\n",
            "Score for fold 4: loss of 0.028589071705937386; accuracy of 99.01348948478699% iou_coef_multilabel of 79.61382269859314% dice_coef_multilabel of 84.92236733436584%\n",
            "Score for fold 5: loss of 0.02963246963918209; accuracy of 99.15727376937866% iou_coef_multilabel of 75.42580366134644% dice_coef_multilabel of 78.91713976860046%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9-gnhHg7mVW"
      },
      "source": [
        "##Model data augmented kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocj4dk1_7uDf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifq3F_K-7mVc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOTJFgoH7mVc"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRrgKSAz7mVc",
        "outputId": "fc0b22b2-44b4-4833-f142-e3b9ea0633f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.030574526637792587; accuracy of 99.18301105499268% iou_coef_multilabel of 77.91773676872253% dice_coef_multilabel of 81.18282556533813%\n",
            "Score for fold 2: loss of 0.0644448772072792; accuracy of 98.29415082931519% iou_coef_multilabel of 74.46072101593018% dice_coef_multilabel of 79.81048822402954%\n",
            "Score for fold 3: loss of 0.06618913263082504; accuracy of 98.34594130516052% iou_coef_multilabel of 73.79118204116821% dice_coef_multilabel of 78.7887454032898%\n",
            "Score for fold 4: loss of 0.05361005291342735; accuracy of 98.4992504119873% iou_coef_multilabel of 76.66850090026855% dice_coef_multilabel of 82.41685032844543%\n",
            "Score for fold 5: loss of 0.06054575368762016; accuracy of 98.66406321525574% iou_coef_multilabel of 70.25530338287354% dice_coef_multilabel of 74.33005571365356%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((X_fold, Y_fold))\n",
        "  # Apply the data augmentation pipeline to both X_train and Y_train\n",
        "  augmented_train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x, y: x)\n",
        "  Y_train_augmented = augmented_train_data.map(lambda x, y: y)\n",
        "\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = np.asarray(list(Y_train_augmented.as_numpy_iterator()))\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr59x7qJ7mVc",
        "outputId": "aa8144c1-3d85-47da-a6b5-a17c4fe445bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.04008716717362404; accuracy of 98.93805980682373% iou_coef_multilabel of 79.03890013694763% dice_coef_multilabel of 82.59339332580566%\n",
            "Score for fold 2: loss of 0.0618879459798336; accuracy of 98.21731448173523% iou_coef_multilabel of 75.36373138427734% dice_coef_multilabel of 80.78001737594604%\n",
            "Score for fold 3: loss of 0.05857839807868004; accuracy of 98.21037650108337% iou_coef_multilabel of 69.53166723251343% dice_coef_multilabel of 75.00591278076172%\n",
            "Score for fold 4: loss of 0.05203528329730034; accuracy of 98.3975350856781% iou_coef_multilabel of 70.11039853096008% dice_coef_multilabel of 76.27233266830444%\n",
            "Score for fold 5: loss of 0.05389085412025452; accuracy of 98.55652451515198% iou_coef_multilabel of 70.12524604797363% dice_coef_multilabel of 74.36990141868591%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((X_fold, Y_fold))\n",
        "  # Apply the data augmentation pipeline to both X_train and Y_train\n",
        "  augmented_train_data = train_data.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x, y: x)\n",
        "  Y_train_augmented = augmented_train_data.map(lambda x, y: y)\n",
        "\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = np.asarray(list(Y_train_augmented.as_numpy_iterator()))\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxF74NqRTnYL"
      },
      "source": [
        "##Model data augmented kfold Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZiNs0HTnYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf9mFXu2TnYR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFb761bWTnYR"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9941b798-afcf-4c81-f5c1-f4ae1d71a1b2",
        "id": "GraQHW6qTnYR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.01687796600162983; accuracy of 99.54043626785278% iou_coef_multilabel of 85.70728302001953% dice_coef_multilabel of 88.2942795753479%\n",
            "Score for fold 2: loss of 0.03296380117535591; accuracy of 99.04602766036987% iou_coef_multilabel of 80.73644638061523% dice_coef_multilabel of 85.05041003227234%\n",
            "Score for fold 3: loss of 0.03169004246592522; accuracy of 99.29794073104858% iou_coef_multilabel of 82.87500143051147% dice_coef_multilabel of 86.73526048660278%\n",
            "Score for fold 4: loss of 0.02590073086321354; accuracy of 99.14000630378723% iou_coef_multilabel of 82.01050758361816% dice_coef_multilabel of 87.04196810722351%\n",
            "Score for fold 5: loss of 0.023470886051654816; accuracy of 99.27202463150024% iou_coef_multilabel of 78.13495993614197% dice_coef_multilabel of 81.73242211341858%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTzHLJ8PTnYR"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdb30c2-2e98-4384-95e1-617a513b5c9d",
        "id": "abm6yrrdTnYS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.015453373081982136; accuracy of 99.50757026672363% iou_coef_multilabel of 82.24123120307922% dice_coef_multilabel of 85.04875302314758%\n",
            "Score for fold 2: loss of 0.04136096313595772; accuracy of 98.85668754577637% iou_coef_multilabel of 78.5542905330658% dice_coef_multilabel of 82.86908864974976%\n",
            "Score for fold 3: loss of 0.026296231895685196; accuracy of 99.25772547721863% iou_coef_multilabel of 79.66246604919434% dice_coef_multilabel of 83.86256098747253%\n",
            "Score for fold 4: loss of 0.023807788267731667; accuracy of 99.12872910499573% iou_coef_multilabel of 77.16266512870789% dice_coef_multilabel of 82.68983960151672%\n",
            "Score for fold 5: loss of 0.04154125601053238; accuracy of 99.09101128578186% iou_coef_multilabel of 78.13253998756409% dice_coef_multilabel of 81.70059323310852%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Score for fold 1: loss of 0.015777496621012688; accuracy of 99.4891345500946% iou_coef_multilabel of 83.78176689147949% dice_coef_multilabel of 86.61818504333496%\n",
        "\n",
        "Score for fold 2: loss of 0.03494168445467949; accuracy of 98.98154139518738% iou_coef_multilabel of 81.28145337104797% dice_coef_multilabel of 85.57209968566895%\n",
        "\n",
        "Score for fold 3: loss of 0.03187831863760948; accuracy of 99.23022985458374% iou_coef_multilabel of 78.91602516174316% dice_coef_multilabel of 82.88114666938782%"
      ],
      "metadata": {
        "id": "bHgVMw0RHdm_"
      }
    }
  ]
}