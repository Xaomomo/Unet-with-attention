{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xaomomo/Unet-with-attention/blob/main/Resnext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcH2AvTh97gF"
      },
      "source": [
        "##Descarga datos\n",
        "Los datos se encuentran en el drive, por lo que usara gdown para sacarlos directamente y no tener que hacer la coneccion, ya que estamos descargando un zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GLXIiagwH3",
        "outputId": "ea6b84ca-bd0c-4689-8d3c-1d7875c79499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\n",
            "To: /content/input.zip\n",
            "100% 367M/367M [00:09<00:00, 38.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1KJPU1D0ewHmU7Jzl1irq3Gb-JkUqV1VR&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8I6BfRhogZ"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Y2Q3Bce47L"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jqAPlaMKtpg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "import PIL\n",
        "from PIL import Image\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/images/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    X[i,:,:,0]=im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST)\n",
        "mas=\"/Amasks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  if x!=\"/content/input/train/Amasks/coronacases_009_149.png\":\n",
        "    im=Image.open(x)\n",
        "    im=np.array(im.resize((IMG_WIDTH,IMG_HEIGHT),resample=PIL.Image.NEAREST))\n",
        "    im[im==85]=1\n",
        "    im[im==170]=2\n",
        "    im[im==255]=3\n",
        "    Y[i,:,:]=im\n",
        "import tensorflow as tf\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))\n",
        "Y=tf.keras.utils.to_categorical(Y)\n",
        "print(Y.shape)\n",
        "print(np.unique(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3a4d1f-03ba-4325-c8a5-65b0ad63e327",
        "id": "mu5trMBgI2Wx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3520, 128, 128)\n",
            "[0. 1. 2. 3.]\n",
            "(3520, 128, 128, 4)\n",
            "[0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747bef4f-b655-498f-f7f0-37d2c273c057",
        "id": "XsDeC0BxI2Wy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002\n",
            "003\n",
            "004\n",
            "005\n",
            "006\n",
            "007\n",
            "008\n",
            "009\n",
            "010\n",
            "10_85902_1\n",
            "10_85902_3\n",
            "14_85914_0\n",
            "27_86410_0\n",
            "29_86490_1\n",
            "29_86491_1\n",
            "36_86526_0\n",
            "40_86625_0\n",
            "4_85506_1\n",
            "7_85703_0\n"
          ]
        }
      ],
      "source": [
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "Siz=0\n",
        "id=0\n",
        "ri=0\n",
        "metadat=[]\n",
        "c=\"001\"\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x.find(\"radiopaedia\") == -1:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\")]\n",
        "  else:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\",9)]\n",
        "  if c!=x:\n",
        "    print(x)\n",
        "    metadat.append([id,Siz,ri])\n",
        "    ri=i\n",
        "    c=x\n",
        "    id+=1\n",
        "    Siz=0\n",
        "  Siz+=1\n",
        "metadat.append([id,Siz,ri])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqn8h312I2Wz"
      },
      "source": [
        "##New Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01362e2b-6c60-41ec-84e9-30835a3e1ebf",
        "id": "L4YXvn3tI2Wz"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  4  5  7  8  9 11 12 13 14 15 17 18 19] [ 3  6 10 16]\n",
            "[ 0  1  3  5  6  7  8  9 10 11 12 13 15 16 18 19] [ 2  4 14 17]\n",
            "[ 2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19] [ 0  1  7 13]\n",
            "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17] [ 9 15 18 19]\n",
            "[ 0  1  2  3  4  6  7  9 10 13 14 15 16 17 18 19] [ 5  8 11 12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "IDX=[x for x in range(10)]\n",
        "IDY=[x for x in range(10)]\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  print(train_index, val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU8hWheeI2W0"
      },
      "outputs": [],
      "source": [
        "for i,x in enumerate(train_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_train=X[ri:ri+size]\n",
        "    Y_train=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_train=np.concatenate((X_train,X[ri:ri+size]),axis=0)\n",
        "    Y_train=np.concatenate((Y_train,Y[ri:ri+size]),axis=0)\n",
        "for i,x in enumerate(val_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_test=X[ri:ri+size]\n",
        "    Y_test=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_test=np.concatenate((X_test,X[ri:ri+size]),axis=0)\n",
        "    Y_test=np.concatenate((Y_test,Y[ri:ri+size]),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Borrar directorio /input en caso de error"
      ],
      "metadata": {
        "id": "3NcX5DC3VvAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/input"
      ],
      "metadata": {
        "id": "-QywVJIaZjHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CambWxdEwn"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUySwAEoH-ko"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kli_5DCBH-kp"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def iou_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
        "  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
        "  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
        "  return iou\n",
        "def dice_coef(y_true, y_pred, smooth=1):\n",
        "  intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
        "  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
        "  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)\n",
        "  return dice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      #None,128,128,1\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      #None,128,128\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += dice_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ],
      "metadata": {
        "id": "LmLKTdyUH-kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iou_coef_multilabel(y_true, y_pred, smooth=1):\n",
        "    dice=0\n",
        "    numLabels=y_true.shape[3]\n",
        "    for index in range(numLabels):\n",
        "      y_t=tf.expand_dims(y_true[:,:,:,index],axis=3)\n",
        "      y_p=tf.expand_dims(y_pred[:,:,:,index],axis=3)\n",
        "      dice += iou_coef(y_t, y_p)\n",
        "    return dice/numLabels # taking average"
      ],
      "metadata": {
        "id": "oopp7EhGH-kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (3, 3), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (1, 1), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,s"
      ],
      "metadata": {
        "id": "MDQHBprOXZXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJgrBPxjiV-c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "def create_model():\n",
        "  g=4\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=g)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=g)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=g)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "  c5,_ = conv_block(p4,[256,256],0.3,group=g)\n",
        "\n",
        "  #Expansive path\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, z4])\n",
        "  c6,_ = conv_block(u6,[128,128],0.2,group=g)\n",
        "\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, z3])\n",
        "  c7,_ = conv_block(u7,[64,64],0.2,group=g)\n",
        "\n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, z2])\n",
        "  c8,_ = conv_block(u8,[32,32],group=g)\n",
        "\n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, z1], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=g)\n",
        "\n",
        "  outputs = tf.keras.layers.Conv2D(4, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',iou_coef_multilabel,dice_coef_multilabel])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFyLMRezeWO"
      },
      "source": [
        "##Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCB8atJ1zda4",
        "outputId": "3cca226b-d16c-4595-f4a5-ff17546c8ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "158/158 [==============================] - 52s 116ms/step - loss: 0.3910 - accuracy: 0.8957 - iou_coef_multilabel: 0.2893 - dice_coef_multilabel: 0.3588\n",
            "Epoch 2/25\n",
            "158/158 [==============================] - 11s 71ms/step - loss: 0.0623 - accuracy: 0.9869 - iou_coef_multilabel: 0.5171 - dice_coef_multilabel: 0.5796\n",
            "Epoch 3/25\n",
            "158/158 [==============================] - 11s 69ms/step - loss: 0.0365 - accuracy: 0.9908 - iou_coef_multilabel: 0.5901 - dice_coef_multilabel: 0.6460\n",
            "Epoch 4/25\n",
            "158/158 [==============================] - 11s 69ms/step - loss: 0.0281 - accuracy: 0.9921 - iou_coef_multilabel: 0.6248 - dice_coef_multilabel: 0.6764\n",
            "Epoch 5/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0243 - accuracy: 0.9927 - iou_coef_multilabel: 0.6444 - dice_coef_multilabel: 0.6932\n",
            "Epoch 6/25\n",
            "158/158 [==============================] - 11s 71ms/step - loss: 0.0220 - accuracy: 0.9931 - iou_coef_multilabel: 0.6582 - dice_coef_multilabel: 0.7051\n",
            "Epoch 7/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0202 - accuracy: 0.9934 - iou_coef_multilabel: 0.6700 - dice_coef_multilabel: 0.7156\n",
            "Epoch 8/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0194 - accuracy: 0.9936 - iou_coef_multilabel: 0.6779 - dice_coef_multilabel: 0.7227\n",
            "Epoch 9/25\n",
            "158/158 [==============================] - 12s 74ms/step - loss: 0.0182 - accuracy: 0.9938 - iou_coef_multilabel: 0.6877 - dice_coef_multilabel: 0.7316\n",
            "Epoch 10/25\n",
            "158/158 [==============================] - 12s 73ms/step - loss: 0.0174 - accuracy: 0.9940 - iou_coef_multilabel: 0.6956 - dice_coef_multilabel: 0.7389\n",
            "Epoch 11/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0169 - accuracy: 0.9941 - iou_coef_multilabel: 0.7019 - dice_coef_multilabel: 0.7446\n",
            "Epoch 12/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0165 - accuracy: 0.9942 - iou_coef_multilabel: 0.7085 - dice_coef_multilabel: 0.7508\n",
            "Epoch 13/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0157 - accuracy: 0.9944 - iou_coef_multilabel: 0.7170 - dice_coef_multilabel: 0.7585\n",
            "Epoch 14/25\n",
            "158/158 [==============================] - 11s 71ms/step - loss: 0.0154 - accuracy: 0.9945 - iou_coef_multilabel: 0.7227 - dice_coef_multilabel: 0.7640\n",
            "Epoch 15/25\n",
            "158/158 [==============================] - 11s 72ms/step - loss: 0.0149 - accuracy: 0.9946 - iou_coef_multilabel: 0.7301 - dice_coef_multilabel: 0.7709\n",
            "Epoch 16/25\n",
            "158/158 [==============================] - 11s 72ms/step - loss: 0.0145 - accuracy: 0.9947 - iou_coef_multilabel: 0.7365 - dice_coef_multilabel: 0.7770\n",
            "Epoch 17/25\n",
            "158/158 [==============================] - 13s 82ms/step - loss: 0.0142 - accuracy: 0.9948 - iou_coef_multilabel: 0.7428 - dice_coef_multilabel: 0.7828\n",
            "Epoch 18/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0140 - accuracy: 0.9949 - iou_coef_multilabel: 0.7465 - dice_coef_multilabel: 0.7864\n",
            "Epoch 19/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0138 - accuracy: 0.9949 - iou_coef_multilabel: 0.7534 - dice_coef_multilabel: 0.7931\n",
            "Epoch 20/25\n",
            "158/158 [==============================] - 12s 75ms/step - loss: 0.0135 - accuracy: 0.9950 - iou_coef_multilabel: 0.7602 - dice_coef_multilabel: 0.7995\n",
            "Epoch 21/25\n",
            "158/158 [==============================] - 12s 75ms/step - loss: 0.0133 - accuracy: 0.9951 - iou_coef_multilabel: 0.7633 - dice_coef_multilabel: 0.8023\n",
            "Epoch 22/25\n",
            "158/158 [==============================] - 12s 74ms/step - loss: 0.0135 - accuracy: 0.9950 - iou_coef_multilabel: 0.7620 - dice_coef_multilabel: 0.8012\n",
            "Epoch 23/25\n",
            "158/158 [==============================] - 12s 74ms/step - loss: 0.0130 - accuracy: 0.9951 - iou_coef_multilabel: 0.7707 - dice_coef_multilabel: 0.8095\n",
            "Epoch 24/25\n",
            "158/158 [==============================] - 11s 70ms/step - loss: 0.0128 - accuracy: 0.9952 - iou_coef_multilabel: 0.7761 - dice_coef_multilabel: 0.8146\n",
            "Epoch 25/25\n",
            "158/158 [==============================] - 11s 72ms/step - loss: 0.0125 - accuracy: 0.9953 - iou_coef_multilabel: 0.7820 - dice_coef_multilabel: 0.8201\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f08e39b50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model=create_model()\n",
        "model.fit(X_train,Y_train,batch_size=16,epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0yz227RzmyI",
        "outputId": "9d8aebd0-a011-4dbe-900d-bb275420f389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.02801677957177162; accuracy of 99.20875430107117% iou_coef_multilabel of 70.94874978065491% dice_coef_multilabel of 74.46155548095703%\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2Sj8PPuJBv"
      },
      "source": [
        "## Model Fit Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh7g3mmhuNPV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7ESdJAuOaW"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25 epochs"
      ],
      "metadata": {
        "id": "erWVgdGfhXKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gaF3LTvOU-",
        "outputId": "f40671ab-9c23-4e0b-9e02-d43e271ad723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.010180109180510044; accuracy of 98.96095395088196% DiceMetric of 90.05458354949951%\n",
            "Score for fold 2: loss of 0.02093512751162052; accuracy of 98.82934093475342% DiceMetric of 87.66295909881592%\n",
            "Score for fold 3: loss of 0.021466268226504326; accuracy of 98.8848626613617% DiceMetric of 84.38072204589844%\n",
            "Score for fold 4: loss of 0.02234414406120777; accuracy of 98.87327551841736% DiceMetric of 86.7185652256012%\n",
            "Score for fold 5: loss of 0.020873798057436943; accuracy of 98.92811179161072% DiceMetric of 85.13778448104858%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50 epochs"
      ],
      "metadata": {
        "id": "uOWim3VxhZsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7om-UlhbP9",
        "outputId": "45008848-1906-4cc1-d106-ccd1b258c596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.026294833049178123; accuracy of 98.93321394920349% DiceMetric of 88.32351565361023%\n",
            "Score for fold 2: loss of 0.03351094573736191; accuracy of 98.8107681274414% DiceMetric of 87.70761489868164%\n",
            "Score for fold 3: loss of 0.03587321937084198; accuracy of 98.87182712554932% DiceMetric of 84.11082625389099%\n",
            "Score for fold 4: loss of 0.03035399504005909; accuracy of 98.86696338653564% DiceMetric of 87.92186379432678%\n",
            "Score for fold 5: loss of 0.02977365255355835; accuracy of 98.92091155052185% DiceMetric of 85.00602841377258%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Actualized kfold"
      ],
      "metadata": {
        "id": "MmyyK6YrXA8Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2D-muOYe_Pj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trswxiGSe_Pl"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "id": "7qUsIi9Se4PD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77720d8-cf76-405a-a865-3eefe98af585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.015655724331736565; accuracy of 99.492347240448% iou_coef_multilabel of 78.48652005195618% dice_coef_multilabel of 81.29676580429077%\n",
            "Score for fold 2: loss of 0.033560991287231445; accuracy of 99.04505610466003% iou_coef_multilabel of 76.57327651977539% dice_coef_multilabel of 80.87905645370483%\n",
            "Score for fold 3: loss of 0.027293214574456215; accuracy of 99.12284016609192% iou_coef_multilabel of 75.1055359840393% dice_coef_multilabel of 79.40298914909363%\n",
            "Score for fold 4: loss of 0.02852698229253292; accuracy of 98.97468090057373% iou_coef_multilabel of 74.88389611244202% dice_coef_multilabel of 80.18468618392944%\n",
            "Score for fold 5: loss of 0.025266241282224655; accuracy of 99.28717613220215% iou_coef_multilabel of 74.9929428100586% dice_coef_multilabel of 78.33638191223145%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "\n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "id": "4u-iwkU7t4ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11ed8b39-d688-47fe-efc6-0d38f3020a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.01602116785943508; accuracy of 99.52024221420288% iou_coef_multilabel of 82.14603662490845% dice_coef_multilabel of 84.78365540504456%\n",
            "Score for fold 2: loss of 0.04220713675022125; accuracy of 98.94644618034363% iou_coef_multilabel of 81.23083114624023% dice_coef_multilabel of 85.52900552749634%\n",
            "Score for fold 3: loss of 0.029687974601984024; accuracy of 99.19074773788452% iou_coef_multilabel of 78.43326330184937% dice_coef_multilabel of 82.49987959861755%\n",
            "Score for fold 4: loss of 0.03447713330388069; accuracy of 98.83654117584229% iou_coef_multilabel of 79.59023714065552% dice_coef_multilabel of 85.02606749534607%\n",
            "Score for fold 5: loss of 0.030355304479599; accuracy of 99.24253225326538% iou_coef_multilabel of 74.45892691612244% dice_coef_multilabel of 77.79458165168762%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxF74NqRTnYL"
      },
      "source": [
        "##Model data augmented kfold Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcZiNs0HTnYR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"vertical\"),\n",
        "  tf.keras.layers.RandomRotation(0.1),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf9mFXu2TnYR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFb761bWTnYR"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c06707-c1de-4c6d-f853-817c356479d5",
        "id": "GraQHW6qTnYR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.014148196205496788; accuracy of 99.54409599304199% iou_coef_multilabel of 83.20818543434143% dice_coef_multilabel of 85.87126731872559%\n",
            "Score for fold 2: loss of 0.040437109768390656; accuracy of 99.01766777038574% iou_coef_multilabel of 81.3275158405304% dice_coef_multilabel of 85.5164885520935%\n",
            "Score for fold 3: loss of 0.030514268204569817; accuracy of 99.27120804786682% iou_coef_multilabel of 81.86497092247009% dice_coef_multilabel of 85.83415746688843%\n",
            "Score for fold 4: loss of 0.024612445384263992; accuracy of 99.15140867233276% iou_coef_multilabel of 80.59779405593872% dice_coef_multilabel of 85.63371300697327%\n",
            "Score for fold 5: loss of 0.025231849402189255; accuracy of 99.32593703269958% iou_coef_multilabel of 80.74448108673096% dice_coef_multilabel of 83.97288918495178%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=50,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTzHLJ8PTnYR"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3f35d9-6d46-4ca7-f827-7c367b5f7640",
        "id": "abm6yrrdTnYS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.013703295961022377; accuracy of 99.53930974006653% iou_coef_multilabel of 82.55035281181335% dice_coef_multilabel of 85.26062369346619%\n",
            "Score for fold 2: loss of 0.04075786843895912; accuracy of 98.90410304069519% iou_coef_multilabel of 77.47979760169983% dice_coef_multilabel of 81.85679316520691%\n",
            "Score for fold 3: loss of 0.029439277946949005; accuracy of 99.13827180862427% iou_coef_multilabel of 79.27287817001343% dice_coef_multilabel of 83.47063660621643%\n",
            "Score for fold 4: loss of 0.024929743260145187; accuracy of 99.10342693328857% iou_coef_multilabel of 80.03262281417847% dice_coef_multilabel of 85.09231805801392%\n",
            "Score for fold 5: loss of 0.022195545956492424; accuracy of 99.34304356575012% iou_coef_multilabel of 76.9676148891449% dice_coef_multilabel of 80.50482273101807%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "  X__2 = np.concatenate([X_fold, Y_fold], axis=-1)\n",
        "  X__2 = tf.data.Dataset.from_tensor_slices((X__2))\n",
        "  augmented_train_data = X__2.map(lambda x: data_augmentation(x, training=True))\n",
        "\n",
        "  # Separate X_train and Y_train from the augmented train data\n",
        "  X_train_augmented = augmented_train_data.map(lambda x: x)\n",
        "  X_train_augmented = np.asarray(list(X_train_augmented.as_numpy_iterator()))\n",
        "  Y_train_augmented = X_train_augmented[:,:,:,1:5]\n",
        "  X_train_augmented = X_train_augmented[:,:,:,0]\n",
        "\n",
        "  X_train_combined = np.concatenate([X_fold, X_train_augmented[:,:,:,None]], axis=0)\n",
        "  Y_train_combined = np.concatenate([Y_fold, Y_train_augmented], axis=0)\n",
        "  model.fit(X_train_combined,Y_train_combined,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}% {model.metrics_names[3]} of {scores[3]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHgVMw0RHdm_"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4JFyLMRezeWO",
        "lc2Sj8PPuJBv"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOmWkkZc6yCShQv0MaTprol",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}